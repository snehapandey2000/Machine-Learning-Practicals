{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe90ec9f",
      "metadata": {
        "id": "fe90ec9f"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3733ae24",
      "metadata": {
        "id": "3733ae24",
        "outputId": "829a05ff-3bfe-43f4-933b-58fb84c55ffc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'11.6'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "torch.version.cuda"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b99a218f",
      "metadata": {
        "id": "b99a218f"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14fe257d",
      "metadata": {
        "id": "14fe257d"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "daa34508",
      "metadata": {
        "id": "daa34508"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import tqdm as notebook_tqdm\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from torch.linalg import eig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d60b79d0",
      "metadata": {
        "id": "d60b79d0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51257ea1",
      "metadata": {
        "id": "51257ea1"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('banknotes.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3da01174",
      "metadata": {
        "id": "3da01174",
        "outputId": "92e279e4-92c6-4535-e026-ab5cc9cb0b1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   conterfeit  Length   Left  Right  Bottom   Top  Diagonal\n",
              "0           0   214.8  131.0  131.1     9.0   9.7     141.0\n",
              "1           0   214.6  129.7  129.7     8.1   9.5     141.7\n",
              "2           0   214.8  129.7  129.7     8.7   9.6     142.2\n",
              "3           0   214.8  129.7  129.6     7.5  10.4     142.0\n",
              "4           0   215.0  129.6  129.7    10.4   7.7     141.8"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d5d0345e-8583-41eb-a998-6437c4cea79b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>conterfeit</th>\n",
              "      <th>Length</th>\n",
              "      <th>Left</th>\n",
              "      <th>Right</th>\n",
              "      <th>Bottom</th>\n",
              "      <th>Top</th>\n",
              "      <th>Diagonal</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>214.8</td>\n",
              "      <td>131.0</td>\n",
              "      <td>131.1</td>\n",
              "      <td>9.0</td>\n",
              "      <td>9.7</td>\n",
              "      <td>141.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>214.6</td>\n",
              "      <td>129.7</td>\n",
              "      <td>129.7</td>\n",
              "      <td>8.1</td>\n",
              "      <td>9.5</td>\n",
              "      <td>141.7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>214.8</td>\n",
              "      <td>129.7</td>\n",
              "      <td>129.7</td>\n",
              "      <td>8.7</td>\n",
              "      <td>9.6</td>\n",
              "      <td>142.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>214.8</td>\n",
              "      <td>129.7</td>\n",
              "      <td>129.6</td>\n",
              "      <td>7.5</td>\n",
              "      <td>10.4</td>\n",
              "      <td>142.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>215.0</td>\n",
              "      <td>129.6</td>\n",
              "      <td>129.7</td>\n",
              "      <td>10.4</td>\n",
              "      <td>7.7</td>\n",
              "      <td>141.8</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d5d0345e-8583-41eb-a998-6437c4cea79b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d5d0345e-8583-41eb-a998-6437c4cea79b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d5d0345e-8583-41eb-a998-6437c4cea79b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = data.sample(frac=1).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "CO4JXXwMceHX"
      },
      "id": "CO4JXXwMceHX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c74a88f4",
      "metadata": {
        "id": "c74a88f4",
        "outputId": "f1f404ca-3d7c-4801-a5f7-a1034e3895d9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 200 entries, 0 to 199\n",
            "Data columns (total 7 columns):\n",
            " #   Column      Non-Null Count  Dtype  \n",
            "---  ------      --------------  -----  \n",
            " 0   conterfeit  200 non-null    int64  \n",
            " 1   Length      200 non-null    float64\n",
            " 2   Left        200 non-null    float64\n",
            " 3   Right       200 non-null    float64\n",
            " 4   Bottom      200 non-null    float64\n",
            " 5   Top         200 non-null    float64\n",
            " 6   Diagonal    200 non-null    float64\n",
            "dtypes: float64(6), int64(1)\n",
            "memory usage: 11.1 KB\n"
          ]
        }
      ],
      "source": [
        "data.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd27714e",
      "metadata": {
        "id": "fd27714e",
        "outputId": "e8b8cac5-d9a7-4eab-dd89-b9ac645888c6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    1\n",
              "1    0\n",
              "2    1\n",
              "3    0\n",
              "4    0\n",
              "Name: conterfeit, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 104
        }
      ],
      "source": [
        "y = data['conterfeit']\n",
        "y.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4753da73",
      "metadata": {
        "id": "4753da73"
      },
      "outputs": [],
      "source": [
        "X = data.drop('conterfeit',axis=1)\n",
        "sc = StandardScaler()\n",
        "x = sc.fit_transform(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a192b0d1",
      "metadata": {
        "id": "a192b0d1",
        "outputId": "c07251e9-5dcc-45f4-9743-a2953217c67d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Length   Left  Right  Bottom   Top  Diagonal\n",
              "0   214.7  130.1  130.2    11.6  10.9     139.1\n",
              "1   215.1  130.3  130.3     9.8   9.5     141.9\n",
              "2   215.0  130.2  130.2    10.6  10.7     139.9\n",
              "3   214.9  130.4  129.7     9.0   9.8     140.9\n",
              "4   215.7  130.8  130.5     9.0  10.1     141.4"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c1ce2609-49fd-4d0c-a0a3-abf1fb94289b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Length</th>\n",
              "      <th>Left</th>\n",
              "      <th>Right</th>\n",
              "      <th>Bottom</th>\n",
              "      <th>Top</th>\n",
              "      <th>Diagonal</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>214.7</td>\n",
              "      <td>130.1</td>\n",
              "      <td>130.2</td>\n",
              "      <td>11.6</td>\n",
              "      <td>10.9</td>\n",
              "      <td>139.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>215.1</td>\n",
              "      <td>130.3</td>\n",
              "      <td>130.3</td>\n",
              "      <td>9.8</td>\n",
              "      <td>9.5</td>\n",
              "      <td>141.9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>215.0</td>\n",
              "      <td>130.2</td>\n",
              "      <td>130.2</td>\n",
              "      <td>10.6</td>\n",
              "      <td>10.7</td>\n",
              "      <td>139.9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>214.9</td>\n",
              "      <td>130.4</td>\n",
              "      <td>129.7</td>\n",
              "      <td>9.0</td>\n",
              "      <td>9.8</td>\n",
              "      <td>140.9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>215.7</td>\n",
              "      <td>130.8</td>\n",
              "      <td>130.5</td>\n",
              "      <td>9.0</td>\n",
              "      <td>10.1</td>\n",
              "      <td>141.4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c1ce2609-49fd-4d0c-a0a3-abf1fb94289b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c1ce2609-49fd-4d0c-a0a3-abf1fb94289b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c1ce2609-49fd-4d0c-a0a3-abf1fb94289b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 106
        }
      ],
      "source": [
        "X.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d449255",
      "metadata": {
        "id": "3d449255",
        "outputId": "7de3c900-26da-45bf-a451-ec03fa61be85",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.52181576, -0.05970201,  0.6041277 ,  1.5145868 ,  0.31151022,\n",
              "        -1.20369089],\n",
              "       [ 0.54311436,  0.49566556,  0.85222942,  0.26544305, -1.43644293,\n",
              "         1.23240199],\n",
              "       [ 0.27688183,  0.21798177,  0.6041277 ,  0.82061805,  0.06180263,\n",
              "        -0.50766436],\n",
              "       ...,\n",
              "       [ 0.0106493 ,  1.05103313,  0.6041277 ,  0.82061805,  1.06063301,\n",
              "        -0.50766436],\n",
              "       [ 0.54311436,  0.21798177, -0.3882792 ,  0.54303055,  1.68490199,\n",
              "        -0.94268094],\n",
              "       [ 1.34181195, -0.89275337, -1.3806861 , -0.9837007 , -0.06305117,\n",
              "         0.88438872]])"
            ]
          },
          "metadata": {},
          "execution_count": 107
        }
      ],
      "source": [
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72c80042",
      "metadata": {
        "id": "72c80042"
      },
      "outputs": [],
      "source": [
        "PyData = torch.tensor(x,dtype = torch.float32)\n",
        "PyY = torch.tensor(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "251ba571",
      "metadata": {
        "id": "251ba571",
        "outputId": "7765c950-e440-41b4-c80d-f62be7e33396",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(device(type='cpu'), device(type='cpu'))"
            ]
          },
          "metadata": {},
          "execution_count": 109
        }
      ],
      "source": [
        "PyData.device, PyY.device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e081b20a",
      "metadata": {
        "id": "e081b20a"
      },
      "outputs": [],
      "source": [
        "PyData = PyData.to(device)\n",
        "PyY = PyY.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f35acd1a",
      "metadata": {
        "id": "f35acd1a",
        "outputId": "676717a9-b5a0-4744-e4c2-99b6fcef6ab2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(device(type='cuda', index=0), device(type='cuda', index=0))"
            ]
          },
          "metadata": {},
          "execution_count": 111
        }
      ],
      "source": [
        "PyData.device, PyY.device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b251e85b",
      "metadata": {
        "id": "b251e85b"
      },
      "outputs": [],
      "source": [
        "# PyData_mean = torch.mean(PyData, dim=0)\n",
        "# PyData_centered = PyData - PyData_mean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e7baccd",
      "metadata": {
        "id": "3e7baccd"
      },
      "outputs": [],
      "source": [
        "pca = torch.pca_lowrank(PyData)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c9b0b41",
      "metadata": {
        "id": "6c9b0b41",
        "outputId": "1b810c19-6e22-4605-eb3d-06264bcf2100",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[ 0.0657, -0.0660,  0.0439,  0.0613, -0.0496, -0.0231],\n",
              "         [-0.0159,  0.0767,  0.1193, -0.0827, -0.0544,  0.0216],\n",
              "         [ 0.0412,  0.0056,  0.0352,  0.0369, -0.0362, -0.0114],\n",
              "         ...,\n",
              "         [ 0.0725,  0.0041, -0.0175, -0.0023,  0.0086,  0.0984],\n",
              "         [ 0.0501, -0.0086, -0.0874,  0.1231,  0.0244,  0.0887],\n",
              "         [-0.0807,  0.0594, -0.0539,  0.1020,  0.0168,  0.0410]],\n",
              "        device='cuda:0'),\n",
              " tensor([24.2716, 15.9880, 13.1836,  9.4844,  7.3304,  6.1462], device='cuda:0'),\n",
              " tensor([[-0.0070,  0.8155, -0.0177,  0.5746, -0.0588, -0.0311],\n",
              "         [ 0.4678,  0.3420,  0.1034, -0.3949,  0.6395,  0.2977],\n",
              "         [ 0.4867,  0.2525,  0.1235, -0.4303, -0.6141, -0.3492],\n",
              "         [ 0.4068, -0.2662,  0.5835,  0.4037, -0.2155,  0.4624],\n",
              "         [ 0.3679, -0.0915, -0.7876,  0.1102, -0.2198,  0.4190],\n",
              "         [-0.4935,  0.2739,  0.1139, -0.3919, -0.3402,  0.6318]],\n",
              "        device='cuda:0'))"
            ]
          },
          "metadata": {},
          "execution_count": 114
        }
      ],
      "source": [
        "pca"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0c15bb5",
      "metadata": {
        "id": "b0c15bb5"
      },
      "outputs": [],
      "source": [
        "pca_x = pca[0][:,:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30b5e787",
      "metadata": {
        "id": "30b5e787"
      },
      "outputs": [],
      "source": [
        "X_train = pca_x[:160]\n",
        "X_test = pca_x[-40:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f696b3e1",
      "metadata": {
        "id": "f696b3e1",
        "outputId": "454a1809-398c-495b-a1d9-69337e6d908b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([40, 5])"
            ]
          },
          "metadata": {},
          "execution_count": 117
        }
      ],
      "source": [
        "X_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28ea61a1",
      "metadata": {
        "id": "28ea61a1"
      },
      "outputs": [],
      "source": [
        "y_train = PyY[:160]\n",
        "y_test = PyY[-40:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ffa191da",
      "metadata": {
        "id": "ffa191da",
        "outputId": "775c54dd-4586-4ec7-f464-ed53bf39f5e0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,\n",
              "        1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 119
        }
      ],
      "source": [
        "y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b51e0497",
      "metadata": {
        "id": "b51e0497"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2dc085d",
      "metadata": {
        "id": "c2dc085d"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import OrderedDict\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "iRV8TibSTl0W"
      },
      "id": "iRV8TibSTl0W",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model(infet=5,hiddDim=8,nbClass=2):\n",
        "  model = nn.Sequential(\n",
        "      OrderedDict([\n",
        "          ('hiddenLayer1', nn.Linear(infet,hiddDim)),\n",
        "          ('activation1',nn.ReLU()),\n",
        "          (\"output_layer\", nn.Linear(hiddDim, nbClass))\n",
        "      ])\n",
        "  )\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "zWES6-UwTw79"
      },
      "id": "zWES6-UwTw79",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model1 = model()\n",
        "mlp = model1.to(device)\n",
        "mlp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Yw6teX0VI2k",
        "outputId": "133d44a7-b4e1-4924-8a1c-2140155aa73d"
      },
      "id": "2Yw6teX0VI2k",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (hiddenLayer1): Linear(in_features=5, out_features=8, bias=True)\n",
              "  (activation1): ReLU()\n",
              "  (output_layer): Linear(in_features=8, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from pyimagesearch import mlp\n",
        "from torch.optim import SGD\n",
        "from sklearn.model_selection import train_test_split\n",
        "# from sklearn.datasets import make_blobs"
      ],
      "metadata": {
        "id": "_d9gNc70V3SM"
      },
      "id": "_d9gNc70V3SM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "opt = SGD(mlp.parameters(),lr=1e-2)\n",
        "lossFunc = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "ozeGv59eVSIu"
      },
      "id": "ozeGv59eVSIu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 300\n",
        "BATCH_SIZE = 10"
      ],
      "metadata": {
        "id": "q6tHT6Yses9v"
      },
      "id": "q6tHT6Yses9v",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def next_batch(inputs, targets, batchSize):\n",
        "\t# loop over the dataset\n",
        "\tfor i in range(0, inputs.shape[0], batchSize):\n",
        "\t\t# yield a tuple of the current batched data and labels\n",
        "\t\tyield (inputs[i:i + batchSize], targets[i:i + batchSize])"
      ],
      "metadata": {
        "id": "kGYfK8jme_3z"
      },
      "id": "kGYfK8jme_3z",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(0, EPOCHS):\n",
        "\t# initialize tracker variables and set our model to trainable\n",
        "\tprint(\"[INFO] epoch: {}...\".format(epoch + 1))\n",
        "\ttrainLoss = 0\n",
        "\ttrainAcc = 0\n",
        "\tsamples = 0\n",
        "\tmlp.train()\n",
        "\t# loop over the current batch of data\n",
        "\tfor (batchX, batchY) in next_batch(X_train, y_train, BATCH_SIZE):\n",
        "\t\t# flash data to the current device, run it through our\n",
        "\t\t# model, and calculate loss\n",
        "\t\t(batchX, batchY) = (batchX.to(device), batchY.to(device))\n",
        "\t\tpredictions = mlp(batchX)\n",
        "\t\tloss = lossFunc(predictions, batchY.long())\n",
        "\t\t# zero the gradients accumulated from the previous steps,\n",
        "\t\t# perform backpropagation, and update model parameters\n",
        "\t\topt.zero_grad()\n",
        "\t\tloss.backward()\n",
        "\t\topt.step()\n",
        "\t\t# update training loss, accuracy, and the number of samples\n",
        "\t\t# visited\n",
        "\t\ttrainLoss += loss.item() * batchY.size(0)\n",
        "\t\ttrainAcc += (predictions.max(1)[1] == batchY).sum().item()\n",
        "\t\tsamples += batchY.size(0)\n",
        "\t# display model progress on the current training batch\n",
        "\ttrainTemplate = \"epoch: {} train loss: {:.3f} train accuracy: {:.3f}\"\n",
        "\tprint(trainTemplate.format(epoch + 1, (trainLoss / samples),\n",
        "\t\t(trainAcc / samples)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JBD5zqxlV1vc",
        "outputId": "68119878-a0d5-43a6-eb9b-d0c989e50d69"
      },
      "id": "JBD5zqxlV1vc",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] epoch: 1...\n",
            "epoch: 1 train loss: 0.696 train accuracy: 0.512\n",
            "[INFO] epoch: 2...\n",
            "epoch: 2 train loss: 0.696 train accuracy: 0.512\n",
            "[INFO] epoch: 3...\n",
            "epoch: 3 train loss: 0.696 train accuracy: 0.512\n",
            "[INFO] epoch: 4...\n",
            "epoch: 4 train loss: 0.696 train accuracy: 0.512\n",
            "[INFO] epoch: 5...\n",
            "epoch: 5 train loss: 0.696 train accuracy: 0.512\n",
            "[INFO] epoch: 6...\n",
            "epoch: 6 train loss: 0.696 train accuracy: 0.512\n",
            "[INFO] epoch: 7...\n",
            "epoch: 7 train loss: 0.696 train accuracy: 0.512\n",
            "[INFO] epoch: 8...\n",
            "epoch: 8 train loss: 0.696 train accuracy: 0.512\n",
            "[INFO] epoch: 9...\n",
            "epoch: 9 train loss: 0.696 train accuracy: 0.512\n",
            "[INFO] epoch: 10...\n",
            "epoch: 10 train loss: 0.695 train accuracy: 0.512\n",
            "[INFO] epoch: 11...\n",
            "epoch: 11 train loss: 0.695 train accuracy: 0.512\n",
            "[INFO] epoch: 12...\n",
            "epoch: 12 train loss: 0.695 train accuracy: 0.512\n",
            "[INFO] epoch: 13...\n",
            "epoch: 13 train loss: 0.695 train accuracy: 0.512\n",
            "[INFO] epoch: 14...\n",
            "epoch: 14 train loss: 0.695 train accuracy: 0.512\n",
            "[INFO] epoch: 15...\n",
            "epoch: 15 train loss: 0.695 train accuracy: 0.512\n",
            "[INFO] epoch: 16...\n",
            "epoch: 16 train loss: 0.695 train accuracy: 0.512\n",
            "[INFO] epoch: 17...\n",
            "epoch: 17 train loss: 0.695 train accuracy: 0.512\n",
            "[INFO] epoch: 18...\n",
            "epoch: 18 train loss: 0.695 train accuracy: 0.512\n",
            "[INFO] epoch: 19...\n",
            "epoch: 19 train loss: 0.695 train accuracy: 0.512\n",
            "[INFO] epoch: 20...\n",
            "epoch: 20 train loss: 0.695 train accuracy: 0.512\n",
            "[INFO] epoch: 21...\n",
            "epoch: 21 train loss: 0.694 train accuracy: 0.512\n",
            "[INFO] epoch: 22...\n",
            "epoch: 22 train loss: 0.694 train accuracy: 0.512\n",
            "[INFO] epoch: 23...\n",
            "epoch: 23 train loss: 0.694 train accuracy: 0.512\n",
            "[INFO] epoch: 24...\n",
            "epoch: 24 train loss: 0.694 train accuracy: 0.512\n",
            "[INFO] epoch: 25...\n",
            "epoch: 25 train loss: 0.694 train accuracy: 0.512\n",
            "[INFO] epoch: 26...\n",
            "epoch: 26 train loss: 0.694 train accuracy: 0.512\n",
            "[INFO] epoch: 27...\n",
            "epoch: 27 train loss: 0.694 train accuracy: 0.512\n",
            "[INFO] epoch: 28...\n",
            "epoch: 28 train loss: 0.694 train accuracy: 0.512\n",
            "[INFO] epoch: 29...\n",
            "epoch: 29 train loss: 0.694 train accuracy: 0.512\n",
            "[INFO] epoch: 30...\n",
            "epoch: 30 train loss: 0.694 train accuracy: 0.512\n",
            "[INFO] epoch: 31...\n",
            "epoch: 31 train loss: 0.693 train accuracy: 0.512\n",
            "[INFO] epoch: 32...\n",
            "epoch: 32 train loss: 0.693 train accuracy: 0.512\n",
            "[INFO] epoch: 33...\n",
            "epoch: 33 train loss: 0.693 train accuracy: 0.512\n",
            "[INFO] epoch: 34...\n",
            "epoch: 34 train loss: 0.693 train accuracy: 0.512\n",
            "[INFO] epoch: 35...\n",
            "epoch: 35 train loss: 0.693 train accuracy: 0.512\n",
            "[INFO] epoch: 36...\n",
            "epoch: 36 train loss: 0.693 train accuracy: 0.512\n",
            "[INFO] epoch: 37...\n",
            "epoch: 37 train loss: 0.693 train accuracy: 0.512\n",
            "[INFO] epoch: 38...\n",
            "epoch: 38 train loss: 0.692 train accuracy: 0.512\n",
            "[INFO] epoch: 39...\n",
            "epoch: 39 train loss: 0.692 train accuracy: 0.512\n",
            "[INFO] epoch: 40...\n",
            "epoch: 40 train loss: 0.692 train accuracy: 0.512\n",
            "[INFO] epoch: 41...\n",
            "epoch: 41 train loss: 0.692 train accuracy: 0.512\n",
            "[INFO] epoch: 42...\n",
            "epoch: 42 train loss: 0.692 train accuracy: 0.512\n",
            "[INFO] epoch: 43...\n",
            "epoch: 43 train loss: 0.692 train accuracy: 0.512\n",
            "[INFO] epoch: 44...\n",
            "epoch: 44 train loss: 0.692 train accuracy: 0.512\n",
            "[INFO] epoch: 45...\n",
            "epoch: 45 train loss: 0.691 train accuracy: 0.512\n",
            "[INFO] epoch: 46...\n",
            "epoch: 46 train loss: 0.691 train accuracy: 0.512\n",
            "[INFO] epoch: 47...\n",
            "epoch: 47 train loss: 0.691 train accuracy: 0.512\n",
            "[INFO] epoch: 48...\n",
            "epoch: 48 train loss: 0.691 train accuracy: 0.512\n",
            "[INFO] epoch: 49...\n",
            "epoch: 49 train loss: 0.691 train accuracy: 0.512\n",
            "[INFO] epoch: 50...\n",
            "epoch: 50 train loss: 0.691 train accuracy: 0.512\n",
            "[INFO] epoch: 51...\n",
            "epoch: 51 train loss: 0.690 train accuracy: 0.512\n",
            "[INFO] epoch: 52...\n",
            "epoch: 52 train loss: 0.690 train accuracy: 0.512\n",
            "[INFO] epoch: 53...\n",
            "epoch: 53 train loss: 0.690 train accuracy: 0.512\n",
            "[INFO] epoch: 54...\n",
            "epoch: 54 train loss: 0.690 train accuracy: 0.512\n",
            "[INFO] epoch: 55...\n",
            "epoch: 55 train loss: 0.690 train accuracy: 0.512\n",
            "[INFO] epoch: 56...\n",
            "epoch: 56 train loss: 0.689 train accuracy: 0.512\n",
            "[INFO] epoch: 57...\n",
            "epoch: 57 train loss: 0.689 train accuracy: 0.512\n",
            "[INFO] epoch: 58...\n",
            "epoch: 58 train loss: 0.689 train accuracy: 0.512\n",
            "[INFO] epoch: 59...\n",
            "epoch: 59 train loss: 0.689 train accuracy: 0.512\n",
            "[INFO] epoch: 60...\n",
            "epoch: 60 train loss: 0.689 train accuracy: 0.512\n",
            "[INFO] epoch: 61...\n",
            "epoch: 61 train loss: 0.688 train accuracy: 0.512\n",
            "[INFO] epoch: 62...\n",
            "epoch: 62 train loss: 0.688 train accuracy: 0.512\n",
            "[INFO] epoch: 63...\n",
            "epoch: 63 train loss: 0.688 train accuracy: 0.512\n",
            "[INFO] epoch: 64...\n",
            "epoch: 64 train loss: 0.688 train accuracy: 0.512\n",
            "[INFO] epoch: 65...\n",
            "epoch: 65 train loss: 0.688 train accuracy: 0.512\n",
            "[INFO] epoch: 66...\n",
            "epoch: 66 train loss: 0.687 train accuracy: 0.512\n",
            "[INFO] epoch: 67...\n",
            "epoch: 67 train loss: 0.687 train accuracy: 0.512\n",
            "[INFO] epoch: 68...\n",
            "epoch: 68 train loss: 0.687 train accuracy: 0.512\n",
            "[INFO] epoch: 69...\n",
            "epoch: 69 train loss: 0.687 train accuracy: 0.512\n",
            "[INFO] epoch: 70...\n",
            "epoch: 70 train loss: 0.686 train accuracy: 0.512\n",
            "[INFO] epoch: 71...\n",
            "epoch: 71 train loss: 0.686 train accuracy: 0.512\n",
            "[INFO] epoch: 72...\n",
            "epoch: 72 train loss: 0.686 train accuracy: 0.512\n",
            "[INFO] epoch: 73...\n",
            "epoch: 73 train loss: 0.686 train accuracy: 0.512\n",
            "[INFO] epoch: 74...\n",
            "epoch: 74 train loss: 0.686 train accuracy: 0.512\n",
            "[INFO] epoch: 75...\n",
            "epoch: 75 train loss: 0.685 train accuracy: 0.512\n",
            "[INFO] epoch: 76...\n",
            "epoch: 76 train loss: 0.685 train accuracy: 0.512\n",
            "[INFO] epoch: 77...\n",
            "epoch: 77 train loss: 0.685 train accuracy: 0.512\n",
            "[INFO] epoch: 78...\n",
            "epoch: 78 train loss: 0.685 train accuracy: 0.512\n",
            "[INFO] epoch: 79...\n",
            "epoch: 79 train loss: 0.684 train accuracy: 0.512\n",
            "[INFO] epoch: 80...\n",
            "epoch: 80 train loss: 0.684 train accuracy: 0.512\n",
            "[INFO] epoch: 81...\n",
            "epoch: 81 train loss: 0.684 train accuracy: 0.512\n",
            "[INFO] epoch: 82...\n",
            "epoch: 82 train loss: 0.684 train accuracy: 0.512\n",
            "[INFO] epoch: 83...\n",
            "epoch: 83 train loss: 0.683 train accuracy: 0.512\n",
            "[INFO] epoch: 84...\n",
            "epoch: 84 train loss: 0.683 train accuracy: 0.512\n",
            "[INFO] epoch: 85...\n",
            "epoch: 85 train loss: 0.683 train accuracy: 0.512\n",
            "[INFO] epoch: 86...\n",
            "epoch: 86 train loss: 0.683 train accuracy: 0.512\n",
            "[INFO] epoch: 87...\n",
            "epoch: 87 train loss: 0.682 train accuracy: 0.512\n",
            "[INFO] epoch: 88...\n",
            "epoch: 88 train loss: 0.682 train accuracy: 0.512\n",
            "[INFO] epoch: 89...\n",
            "epoch: 89 train loss: 0.682 train accuracy: 0.512\n",
            "[INFO] epoch: 90...\n",
            "epoch: 90 train loss: 0.682 train accuracy: 0.512\n",
            "[INFO] epoch: 91...\n",
            "epoch: 91 train loss: 0.681 train accuracy: 0.512\n",
            "[INFO] epoch: 92...\n",
            "epoch: 92 train loss: 0.681 train accuracy: 0.512\n",
            "[INFO] epoch: 93...\n",
            "epoch: 93 train loss: 0.681 train accuracy: 0.512\n",
            "[INFO] epoch: 94...\n",
            "epoch: 94 train loss: 0.681 train accuracy: 0.519\n",
            "[INFO] epoch: 95...\n",
            "epoch: 95 train loss: 0.680 train accuracy: 0.525\n",
            "[INFO] epoch: 96...\n",
            "epoch: 96 train loss: 0.680 train accuracy: 0.531\n",
            "[INFO] epoch: 97...\n",
            "epoch: 97 train loss: 0.680 train accuracy: 0.537\n",
            "[INFO] epoch: 98...\n",
            "epoch: 98 train loss: 0.680 train accuracy: 0.544\n",
            "[INFO] epoch: 99...\n",
            "epoch: 99 train loss: 0.679 train accuracy: 0.544\n",
            "[INFO] epoch: 100...\n",
            "epoch: 100 train loss: 0.679 train accuracy: 0.544\n",
            "[INFO] epoch: 101...\n",
            "epoch: 101 train loss: 0.679 train accuracy: 0.544\n",
            "[INFO] epoch: 102...\n",
            "epoch: 102 train loss: 0.679 train accuracy: 0.550\n",
            "[INFO] epoch: 103...\n",
            "epoch: 103 train loss: 0.678 train accuracy: 0.550\n",
            "[INFO] epoch: 104...\n",
            "epoch: 104 train loss: 0.678 train accuracy: 0.550\n",
            "[INFO] epoch: 105...\n",
            "epoch: 105 train loss: 0.678 train accuracy: 0.556\n",
            "[INFO] epoch: 106...\n",
            "epoch: 106 train loss: 0.677 train accuracy: 0.562\n",
            "[INFO] epoch: 107...\n",
            "epoch: 107 train loss: 0.677 train accuracy: 0.569\n",
            "[INFO] epoch: 108...\n",
            "epoch: 108 train loss: 0.677 train accuracy: 0.562\n",
            "[INFO] epoch: 109...\n",
            "epoch: 109 train loss: 0.676 train accuracy: 0.569\n",
            "[INFO] epoch: 110...\n",
            "epoch: 110 train loss: 0.676 train accuracy: 0.569\n",
            "[INFO] epoch: 111...\n",
            "epoch: 111 train loss: 0.676 train accuracy: 0.569\n",
            "[INFO] epoch: 112...\n",
            "epoch: 112 train loss: 0.676 train accuracy: 0.569\n",
            "[INFO] epoch: 113...\n",
            "epoch: 113 train loss: 0.675 train accuracy: 0.581\n",
            "[INFO] epoch: 114...\n",
            "epoch: 114 train loss: 0.675 train accuracy: 0.600\n",
            "[INFO] epoch: 115...\n",
            "epoch: 115 train loss: 0.675 train accuracy: 0.606\n",
            "[INFO] epoch: 116...\n",
            "epoch: 116 train loss: 0.674 train accuracy: 0.619\n",
            "[INFO] epoch: 117...\n",
            "epoch: 117 train loss: 0.674 train accuracy: 0.625\n",
            "[INFO] epoch: 118...\n",
            "epoch: 118 train loss: 0.674 train accuracy: 0.625\n",
            "[INFO] epoch: 119...\n",
            "epoch: 119 train loss: 0.673 train accuracy: 0.631\n",
            "[INFO] epoch: 120...\n",
            "epoch: 120 train loss: 0.673 train accuracy: 0.631\n",
            "[INFO] epoch: 121...\n",
            "epoch: 121 train loss: 0.673 train accuracy: 0.631\n",
            "[INFO] epoch: 122...\n",
            "epoch: 122 train loss: 0.672 train accuracy: 0.631\n",
            "[INFO] epoch: 123...\n",
            "epoch: 123 train loss: 0.672 train accuracy: 0.637\n",
            "[INFO] epoch: 124...\n",
            "epoch: 124 train loss: 0.672 train accuracy: 0.637\n",
            "[INFO] epoch: 125...\n",
            "epoch: 125 train loss: 0.671 train accuracy: 0.644\n",
            "[INFO] epoch: 126...\n",
            "epoch: 126 train loss: 0.671 train accuracy: 0.644\n",
            "[INFO] epoch: 127...\n",
            "epoch: 127 train loss: 0.670 train accuracy: 0.656\n",
            "[INFO] epoch: 128...\n",
            "epoch: 128 train loss: 0.670 train accuracy: 0.662\n",
            "[INFO] epoch: 129...\n",
            "epoch: 129 train loss: 0.670 train accuracy: 0.662\n",
            "[INFO] epoch: 130...\n",
            "epoch: 130 train loss: 0.669 train accuracy: 0.675\n",
            "[INFO] epoch: 131...\n",
            "epoch: 131 train loss: 0.669 train accuracy: 0.681\n",
            "[INFO] epoch: 132...\n",
            "epoch: 132 train loss: 0.669 train accuracy: 0.688\n",
            "[INFO] epoch: 133...\n",
            "epoch: 133 train loss: 0.668 train accuracy: 0.688\n",
            "[INFO] epoch: 134...\n",
            "epoch: 134 train loss: 0.668 train accuracy: 0.694\n",
            "[INFO] epoch: 135...\n",
            "epoch: 135 train loss: 0.667 train accuracy: 0.700\n",
            "[INFO] epoch: 136...\n",
            "epoch: 136 train loss: 0.667 train accuracy: 0.700\n",
            "[INFO] epoch: 137...\n",
            "epoch: 137 train loss: 0.667 train accuracy: 0.700\n",
            "[INFO] epoch: 138...\n",
            "epoch: 138 train loss: 0.666 train accuracy: 0.700\n",
            "[INFO] epoch: 139...\n",
            "epoch: 139 train loss: 0.666 train accuracy: 0.700\n",
            "[INFO] epoch: 140...\n",
            "epoch: 140 train loss: 0.665 train accuracy: 0.700\n",
            "[INFO] epoch: 141...\n",
            "epoch: 141 train loss: 0.665 train accuracy: 0.700\n",
            "[INFO] epoch: 142...\n",
            "epoch: 142 train loss: 0.664 train accuracy: 0.700\n",
            "[INFO] epoch: 143...\n",
            "epoch: 143 train loss: 0.664 train accuracy: 0.706\n",
            "[INFO] epoch: 144...\n",
            "epoch: 144 train loss: 0.664 train accuracy: 0.706\n",
            "[INFO] epoch: 145...\n",
            "epoch: 145 train loss: 0.663 train accuracy: 0.713\n",
            "[INFO] epoch: 146...\n",
            "epoch: 146 train loss: 0.663 train accuracy: 0.713\n",
            "[INFO] epoch: 147...\n",
            "epoch: 147 train loss: 0.662 train accuracy: 0.713\n",
            "[INFO] epoch: 148...\n",
            "epoch: 148 train loss: 0.662 train accuracy: 0.713\n",
            "[INFO] epoch: 149...\n",
            "epoch: 149 train loss: 0.661 train accuracy: 0.713\n",
            "[INFO] epoch: 150...\n",
            "epoch: 150 train loss: 0.661 train accuracy: 0.719\n",
            "[INFO] epoch: 151...\n",
            "epoch: 151 train loss: 0.660 train accuracy: 0.719\n",
            "[INFO] epoch: 152...\n",
            "epoch: 152 train loss: 0.660 train accuracy: 0.719\n",
            "[INFO] epoch: 153...\n",
            "epoch: 153 train loss: 0.659 train accuracy: 0.719\n",
            "[INFO] epoch: 154...\n",
            "epoch: 154 train loss: 0.659 train accuracy: 0.731\n",
            "[INFO] epoch: 155...\n",
            "epoch: 155 train loss: 0.658 train accuracy: 0.731\n",
            "[INFO] epoch: 156...\n",
            "epoch: 156 train loss: 0.658 train accuracy: 0.738\n",
            "[INFO] epoch: 157...\n",
            "epoch: 157 train loss: 0.657 train accuracy: 0.744\n",
            "[INFO] epoch: 158...\n",
            "epoch: 158 train loss: 0.657 train accuracy: 0.756\n",
            "[INFO] epoch: 159...\n",
            "epoch: 159 train loss: 0.656 train accuracy: 0.756\n",
            "[INFO] epoch: 160...\n",
            "epoch: 160 train loss: 0.656 train accuracy: 0.756\n",
            "[INFO] epoch: 161...\n",
            "epoch: 161 train loss: 0.655 train accuracy: 0.756\n",
            "[INFO] epoch: 162...\n",
            "epoch: 162 train loss: 0.655 train accuracy: 0.756\n",
            "[INFO] epoch: 163...\n",
            "epoch: 163 train loss: 0.654 train accuracy: 0.769\n",
            "[INFO] epoch: 164...\n",
            "epoch: 164 train loss: 0.653 train accuracy: 0.775\n",
            "[INFO] epoch: 165...\n",
            "epoch: 165 train loss: 0.653 train accuracy: 0.775\n",
            "[INFO] epoch: 166...\n",
            "epoch: 166 train loss: 0.652 train accuracy: 0.781\n",
            "[INFO] epoch: 167...\n",
            "epoch: 167 train loss: 0.652 train accuracy: 0.781\n",
            "[INFO] epoch: 168...\n",
            "epoch: 168 train loss: 0.651 train accuracy: 0.781\n",
            "[INFO] epoch: 169...\n",
            "epoch: 169 train loss: 0.650 train accuracy: 0.787\n",
            "[INFO] epoch: 170...\n",
            "epoch: 170 train loss: 0.650 train accuracy: 0.787\n",
            "[INFO] epoch: 171...\n",
            "epoch: 171 train loss: 0.649 train accuracy: 0.787\n",
            "[INFO] epoch: 172...\n",
            "epoch: 172 train loss: 0.649 train accuracy: 0.787\n",
            "[INFO] epoch: 173...\n",
            "epoch: 173 train loss: 0.648 train accuracy: 0.794\n",
            "[INFO] epoch: 174...\n",
            "epoch: 174 train loss: 0.647 train accuracy: 0.794\n",
            "[INFO] epoch: 175...\n",
            "epoch: 175 train loss: 0.647 train accuracy: 0.794\n",
            "[INFO] epoch: 176...\n",
            "epoch: 176 train loss: 0.646 train accuracy: 0.794\n",
            "[INFO] epoch: 177...\n",
            "epoch: 177 train loss: 0.645 train accuracy: 0.800\n",
            "[INFO] epoch: 178...\n",
            "epoch: 178 train loss: 0.645 train accuracy: 0.800\n",
            "[INFO] epoch: 179...\n",
            "epoch: 179 train loss: 0.644 train accuracy: 0.800\n",
            "[INFO] epoch: 180...\n",
            "epoch: 180 train loss: 0.643 train accuracy: 0.800\n",
            "[INFO] epoch: 181...\n",
            "epoch: 181 train loss: 0.643 train accuracy: 0.800\n",
            "[INFO] epoch: 182...\n",
            "epoch: 182 train loss: 0.642 train accuracy: 0.806\n",
            "[INFO] epoch: 183...\n",
            "epoch: 183 train loss: 0.641 train accuracy: 0.806\n",
            "[INFO] epoch: 184...\n",
            "epoch: 184 train loss: 0.640 train accuracy: 0.812\n",
            "[INFO] epoch: 185...\n",
            "epoch: 185 train loss: 0.640 train accuracy: 0.812\n",
            "[INFO] epoch: 186...\n",
            "epoch: 186 train loss: 0.639 train accuracy: 0.812\n",
            "[INFO] epoch: 187...\n",
            "epoch: 187 train loss: 0.638 train accuracy: 0.819\n",
            "[INFO] epoch: 188...\n",
            "epoch: 188 train loss: 0.638 train accuracy: 0.819\n",
            "[INFO] epoch: 189...\n",
            "epoch: 189 train loss: 0.637 train accuracy: 0.819\n",
            "[INFO] epoch: 190...\n",
            "epoch: 190 train loss: 0.636 train accuracy: 0.819\n",
            "[INFO] epoch: 191...\n",
            "epoch: 191 train loss: 0.635 train accuracy: 0.819\n",
            "[INFO] epoch: 192...\n",
            "epoch: 192 train loss: 0.634 train accuracy: 0.819\n",
            "[INFO] epoch: 193...\n",
            "epoch: 193 train loss: 0.634 train accuracy: 0.825\n",
            "[INFO] epoch: 194...\n",
            "epoch: 194 train loss: 0.633 train accuracy: 0.831\n",
            "[INFO] epoch: 195...\n",
            "epoch: 195 train loss: 0.632 train accuracy: 0.838\n",
            "[INFO] epoch: 196...\n",
            "epoch: 196 train loss: 0.631 train accuracy: 0.838\n",
            "[INFO] epoch: 197...\n",
            "epoch: 197 train loss: 0.630 train accuracy: 0.838\n",
            "[INFO] epoch: 198...\n",
            "epoch: 198 train loss: 0.630 train accuracy: 0.838\n",
            "[INFO] epoch: 199...\n",
            "epoch: 199 train loss: 0.629 train accuracy: 0.844\n",
            "[INFO] epoch: 200...\n",
            "epoch: 200 train loss: 0.628 train accuracy: 0.844\n",
            "[INFO] epoch: 201...\n",
            "epoch: 201 train loss: 0.627 train accuracy: 0.844\n",
            "[INFO] epoch: 202...\n",
            "epoch: 202 train loss: 0.626 train accuracy: 0.844\n",
            "[INFO] epoch: 203...\n",
            "epoch: 203 train loss: 0.625 train accuracy: 0.844\n",
            "[INFO] epoch: 204...\n",
            "epoch: 204 train loss: 0.624 train accuracy: 0.844\n",
            "[INFO] epoch: 205...\n",
            "epoch: 205 train loss: 0.623 train accuracy: 0.850\n",
            "[INFO] epoch: 206...\n",
            "epoch: 206 train loss: 0.622 train accuracy: 0.863\n",
            "[INFO] epoch: 207...\n",
            "epoch: 207 train loss: 0.622 train accuracy: 0.863\n",
            "[INFO] epoch: 208...\n",
            "epoch: 208 train loss: 0.621 train accuracy: 0.863\n",
            "[INFO] epoch: 209...\n",
            "epoch: 209 train loss: 0.620 train accuracy: 0.863\n",
            "[INFO] epoch: 210...\n",
            "epoch: 210 train loss: 0.619 train accuracy: 0.863\n",
            "[INFO] epoch: 211...\n",
            "epoch: 211 train loss: 0.618 train accuracy: 0.863\n",
            "[INFO] epoch: 212...\n",
            "epoch: 212 train loss: 0.617 train accuracy: 0.863\n",
            "[INFO] epoch: 213...\n",
            "epoch: 213 train loss: 0.616 train accuracy: 0.863\n",
            "[INFO] epoch: 214...\n",
            "epoch: 214 train loss: 0.615 train accuracy: 0.863\n",
            "[INFO] epoch: 215...\n",
            "epoch: 215 train loss: 0.614 train accuracy: 0.863\n",
            "[INFO] epoch: 216...\n",
            "epoch: 216 train loss: 0.613 train accuracy: 0.863\n",
            "[INFO] epoch: 217...\n",
            "epoch: 217 train loss: 0.612 train accuracy: 0.863\n",
            "[INFO] epoch: 218...\n",
            "epoch: 218 train loss: 0.611 train accuracy: 0.863\n",
            "[INFO] epoch: 219...\n",
            "epoch: 219 train loss: 0.610 train accuracy: 0.863\n",
            "[INFO] epoch: 220...\n",
            "epoch: 220 train loss: 0.609 train accuracy: 0.863\n",
            "[INFO] epoch: 221...\n",
            "epoch: 221 train loss: 0.607 train accuracy: 0.863\n",
            "[INFO] epoch: 222...\n",
            "epoch: 222 train loss: 0.606 train accuracy: 0.869\n",
            "[INFO] epoch: 223...\n",
            "epoch: 223 train loss: 0.605 train accuracy: 0.875\n",
            "[INFO] epoch: 224...\n",
            "epoch: 224 train loss: 0.604 train accuracy: 0.881\n",
            "[INFO] epoch: 225...\n",
            "epoch: 225 train loss: 0.603 train accuracy: 0.881\n",
            "[INFO] epoch: 226...\n",
            "epoch: 226 train loss: 0.602 train accuracy: 0.887\n",
            "[INFO] epoch: 227...\n",
            "epoch: 227 train loss: 0.601 train accuracy: 0.894\n",
            "[INFO] epoch: 228...\n",
            "epoch: 228 train loss: 0.600 train accuracy: 0.894\n",
            "[INFO] epoch: 229...\n",
            "epoch: 229 train loss: 0.598 train accuracy: 0.906\n",
            "[INFO] epoch: 230...\n",
            "epoch: 230 train loss: 0.597 train accuracy: 0.906\n",
            "[INFO] epoch: 231...\n",
            "epoch: 231 train loss: 0.596 train accuracy: 0.906\n",
            "[INFO] epoch: 232...\n",
            "epoch: 232 train loss: 0.595 train accuracy: 0.912\n",
            "[INFO] epoch: 233...\n",
            "epoch: 233 train loss: 0.594 train accuracy: 0.912\n",
            "[INFO] epoch: 234...\n",
            "epoch: 234 train loss: 0.592 train accuracy: 0.919\n",
            "[INFO] epoch: 235...\n",
            "epoch: 235 train loss: 0.591 train accuracy: 0.919\n",
            "[INFO] epoch: 236...\n",
            "epoch: 236 train loss: 0.590 train accuracy: 0.925\n",
            "[INFO] epoch: 237...\n",
            "epoch: 237 train loss: 0.589 train accuracy: 0.931\n",
            "[INFO] epoch: 238...\n",
            "epoch: 238 train loss: 0.587 train accuracy: 0.931\n",
            "[INFO] epoch: 239...\n",
            "epoch: 239 train loss: 0.586 train accuracy: 0.931\n",
            "[INFO] epoch: 240...\n",
            "epoch: 240 train loss: 0.585 train accuracy: 0.931\n",
            "[INFO] epoch: 241...\n",
            "epoch: 241 train loss: 0.584 train accuracy: 0.931\n",
            "[INFO] epoch: 242...\n",
            "epoch: 242 train loss: 0.582 train accuracy: 0.931\n",
            "[INFO] epoch: 243...\n",
            "epoch: 243 train loss: 0.581 train accuracy: 0.931\n",
            "[INFO] epoch: 244...\n",
            "epoch: 244 train loss: 0.580 train accuracy: 0.938\n",
            "[INFO] epoch: 245...\n",
            "epoch: 245 train loss: 0.578 train accuracy: 0.944\n",
            "[INFO] epoch: 246...\n",
            "epoch: 246 train loss: 0.577 train accuracy: 0.944\n",
            "[INFO] epoch: 247...\n",
            "epoch: 247 train loss: 0.576 train accuracy: 0.944\n",
            "[INFO] epoch: 248...\n",
            "epoch: 248 train loss: 0.574 train accuracy: 0.944\n",
            "[INFO] epoch: 249...\n",
            "epoch: 249 train loss: 0.573 train accuracy: 0.950\n",
            "[INFO] epoch: 250...\n",
            "epoch: 250 train loss: 0.571 train accuracy: 0.950\n",
            "[INFO] epoch: 251...\n",
            "epoch: 251 train loss: 0.570 train accuracy: 0.956\n",
            "[INFO] epoch: 252...\n",
            "epoch: 252 train loss: 0.568 train accuracy: 0.956\n",
            "[INFO] epoch: 253...\n",
            "epoch: 253 train loss: 0.567 train accuracy: 0.956\n",
            "[INFO] epoch: 254...\n",
            "epoch: 254 train loss: 0.566 train accuracy: 0.963\n",
            "[INFO] epoch: 255...\n",
            "epoch: 255 train loss: 0.564 train accuracy: 0.963\n",
            "[INFO] epoch: 256...\n",
            "epoch: 256 train loss: 0.563 train accuracy: 0.963\n",
            "[INFO] epoch: 257...\n",
            "epoch: 257 train loss: 0.561 train accuracy: 0.963\n",
            "[INFO] epoch: 258...\n",
            "epoch: 258 train loss: 0.560 train accuracy: 0.963\n",
            "[INFO] epoch: 259...\n",
            "epoch: 259 train loss: 0.558 train accuracy: 0.969\n",
            "[INFO] epoch: 260...\n",
            "epoch: 260 train loss: 0.557 train accuracy: 0.969\n",
            "[INFO] epoch: 261...\n",
            "epoch: 261 train loss: 0.555 train accuracy: 0.969\n",
            "[INFO] epoch: 262...\n",
            "epoch: 262 train loss: 0.554 train accuracy: 0.969\n",
            "[INFO] epoch: 263...\n",
            "epoch: 263 train loss: 0.552 train accuracy: 0.969\n",
            "[INFO] epoch: 264...\n",
            "epoch: 264 train loss: 0.550 train accuracy: 0.969\n",
            "[INFO] epoch: 265...\n",
            "epoch: 265 train loss: 0.549 train accuracy: 0.969\n",
            "[INFO] epoch: 266...\n",
            "epoch: 266 train loss: 0.547 train accuracy: 0.969\n",
            "[INFO] epoch: 267...\n",
            "epoch: 267 train loss: 0.546 train accuracy: 0.969\n",
            "[INFO] epoch: 268...\n",
            "epoch: 268 train loss: 0.544 train accuracy: 0.969\n",
            "[INFO] epoch: 269...\n",
            "epoch: 269 train loss: 0.542 train accuracy: 0.969\n",
            "[INFO] epoch: 270...\n",
            "epoch: 270 train loss: 0.541 train accuracy: 0.969\n",
            "[INFO] epoch: 271...\n",
            "epoch: 271 train loss: 0.539 train accuracy: 0.969\n",
            "[INFO] epoch: 272...\n",
            "epoch: 272 train loss: 0.538 train accuracy: 0.969\n",
            "[INFO] epoch: 273...\n",
            "epoch: 273 train loss: 0.536 train accuracy: 0.969\n",
            "[INFO] epoch: 274...\n",
            "epoch: 274 train loss: 0.534 train accuracy: 0.969\n",
            "[INFO] epoch: 275...\n",
            "epoch: 275 train loss: 0.532 train accuracy: 0.969\n",
            "[INFO] epoch: 276...\n",
            "epoch: 276 train loss: 0.531 train accuracy: 0.969\n",
            "[INFO] epoch: 277...\n",
            "epoch: 277 train loss: 0.529 train accuracy: 0.969\n",
            "[INFO] epoch: 278...\n",
            "epoch: 278 train loss: 0.527 train accuracy: 0.969\n",
            "[INFO] epoch: 279...\n",
            "epoch: 279 train loss: 0.526 train accuracy: 0.969\n",
            "[INFO] epoch: 280...\n",
            "epoch: 280 train loss: 0.524 train accuracy: 0.975\n",
            "[INFO] epoch: 281...\n",
            "epoch: 281 train loss: 0.522 train accuracy: 0.975\n",
            "[INFO] epoch: 282...\n",
            "epoch: 282 train loss: 0.520 train accuracy: 0.975\n",
            "[INFO] epoch: 283...\n",
            "epoch: 283 train loss: 0.519 train accuracy: 0.975\n",
            "[INFO] epoch: 284...\n",
            "epoch: 284 train loss: 0.517 train accuracy: 0.975\n",
            "[INFO] epoch: 285...\n",
            "epoch: 285 train loss: 0.515 train accuracy: 0.975\n",
            "[INFO] epoch: 286...\n",
            "epoch: 286 train loss: 0.513 train accuracy: 0.975\n",
            "[INFO] epoch: 287...\n",
            "epoch: 287 train loss: 0.511 train accuracy: 0.975\n",
            "[INFO] epoch: 288...\n",
            "epoch: 288 train loss: 0.510 train accuracy: 0.975\n",
            "[INFO] epoch: 289...\n",
            "epoch: 289 train loss: 0.508 train accuracy: 0.981\n",
            "[INFO] epoch: 290...\n",
            "epoch: 290 train loss: 0.506 train accuracy: 0.981\n",
            "[INFO] epoch: 291...\n",
            "epoch: 291 train loss: 0.504 train accuracy: 0.981\n",
            "[INFO] epoch: 292...\n",
            "epoch: 292 train loss: 0.502 train accuracy: 0.981\n",
            "[INFO] epoch: 293...\n",
            "epoch: 293 train loss: 0.500 train accuracy: 0.981\n",
            "[INFO] epoch: 294...\n",
            "epoch: 294 train loss: 0.498 train accuracy: 0.981\n",
            "[INFO] epoch: 295...\n",
            "epoch: 295 train loss: 0.497 train accuracy: 0.981\n",
            "[INFO] epoch: 296...\n",
            "epoch: 296 train loss: 0.495 train accuracy: 0.981\n",
            "[INFO] epoch: 297...\n",
            "epoch: 297 train loss: 0.493 train accuracy: 0.981\n",
            "[INFO] epoch: 298...\n",
            "epoch: 298 train loss: 0.491 train accuracy: 0.988\n",
            "[INFO] epoch: 299...\n",
            "epoch: 299 train loss: 0.489 train accuracy: 0.988\n",
            "[INFO] epoch: 300...\n",
            "epoch: 300 train loss: 0.487 train accuracy: 0.988\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred = mlp(X_test)"
      ],
      "metadata": {
        "id": "KakebkiVfOmU"
      },
      "id": "KakebkiVfOmU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred = pred.max(1)[1]\n",
        "pred"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w7MdPtBcUtS4",
        "outputId": "b47080e8-1204-49ea-82a2-d2a8533230a6"
      },
      "id": "w7MdPtBcUtS4",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,\n",
              "        1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vKnFN4YEVlxh",
        "outputId": "cba51e04-ba66-4284-d23a-e4fbea1106e9"
      },
      "id": "vKnFN4YEVlxh",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,\n",
              "        1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 130
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "ZEr03kvhW_7i"
      },
      "id": "ZEr03kvhW_7i",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_true = y_test.to('cpu')\n",
        "y_pred = pred.to('cpu')\n",
        "y_true"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5HXt3yt8Xe5U",
        "outputId": "45e0e8b2-9ddf-4afb-d27f-24418ca396a3"
      },
      "id": "5HXt3yt8Xe5U",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,\n",
              "        1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 132
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_true = y_true.numpy()\n",
        "y_pred = y_pred.numpy()"
      ],
      "metadata": {
        "id": "UB7AZi5LavYR"
      },
      "id": "UB7AZi5LavYR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cf_matrix = confusion_matrix(y_true, y_pred)\n",
        "cf_matrix"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U28utq--YsFO",
        "outputId": "1d7ca13e-7616-4d0f-dabf-0d5bcf82acf7"
      },
      "id": "U28utq--YsFO",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[21,  1],\n",
              "       [ 0, 18]])"
            ]
          },
          "metadata": {},
          "execution_count": 134
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classes = ['Not Counterfiet','Counterfiet']"
      ],
      "metadata": {
        "id": "cwTFJfstbz1V"
      },
      "id": "cwTFJfstbz1V",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_cm = pd.DataFrame(cf_matrix / np.sum(cf_matrix, axis=1)[:, None], index = [i for i in classes],\n",
        "                     columns = [i for i in classes])\n",
        "plt.figure(figsize = (12,7))\n",
        "sn.heatmap(df_cm, annot=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "M0UU1S4faUQR",
        "outputId": "6a962f44-c8fa-4c8a-fff2-e43661f39ddd"
      },
      "id": "M0UU1S4faUQR",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Axes: >"
            ]
          },
          "metadata": {},
          "execution_count": 136
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 864x504 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iGHMY6gEb5aC"
      },
      "id": "iGHMY6gEb5aC",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}