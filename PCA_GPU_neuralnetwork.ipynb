{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe90ec9f",
      "metadata": {
        "id": "fe90ec9f"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3733ae24",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "3733ae24",
        "outputId": "225cc2b7-75c6-4f2c-bcee-79cf04057d75"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'11.8'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.version.cuda"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ePkiXGtHWEtP",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "id": "ePkiXGtHWEtP",
        "outputId": "a80f32f5-4101-4673-e059-9725b86513d9"
      },
      "outputs": [
        {
          "ename": "MessageError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-d5df0069828e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    130\u001b[0m   )\n\u001b[1;32m    131\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    175\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m   )\n\u001b[0;32m--> 177\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    102\u001b[0m     ):\n\u001b[1;32m    103\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b99a218f",
      "metadata": {
        "id": "b99a218f"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14fe257d",
      "metadata": {
        "id": "14fe257d"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "daa34508",
      "metadata": {
        "id": "daa34508"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import tqdm as notebook_tqdm\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from torch.linalg import eig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d60b79d0",
      "metadata": {
        "id": "d60b79d0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51257ea1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "id": "51257ea1",
        "outputId": "852a72df-5a51-4d5e-8b4e-25ba658e5161"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-5a4a3848204c>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'banknotes.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    676\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    930\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 932\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    933\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    934\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1214\u001b[0m             \u001b[0;31m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0;31m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1216\u001b[0;31m             self.handles = get_handle(  # type: ignore[call-overload]\n\u001b[0m\u001b[1;32m   1217\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    784\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 786\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    787\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'banknotes.csv'"
          ]
        }
      ],
      "source": [
        "data = pd.read_csv('banknotes.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3da01174",
      "metadata": {
        "id": "3da01174"
      },
      "outputs": [],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CO4JXXwMceHX",
      "metadata": {
        "id": "CO4JXXwMceHX"
      },
      "outputs": [],
      "source": [
        "data = data.sample(frac=1).reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c74a88f4",
      "metadata": {
        "id": "c74a88f4"
      },
      "outputs": [],
      "source": [
        "data.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd27714e",
      "metadata": {
        "id": "fd27714e"
      },
      "outputs": [],
      "source": [
        "y = data['conterfeit']\n",
        "y.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4753da73",
      "metadata": {
        "id": "4753da73"
      },
      "outputs": [],
      "source": [
        "X = data.drop('conterfeit',axis=1)\n",
        "sc = StandardScaler()\n",
        "x = sc.fit_transform(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a192b0d1",
      "metadata": {
        "id": "a192b0d1"
      },
      "outputs": [],
      "source": [
        "X.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d449255",
      "metadata": {
        "id": "3d449255"
      },
      "outputs": [],
      "source": [
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72c80042",
      "metadata": {
        "id": "72c80042"
      },
      "outputs": [],
      "source": [
        "PyData = torch.tensor(x,dtype = torch.float32)\n",
        "PyY = torch.tensor(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "251ba571",
      "metadata": {
        "id": "251ba571"
      },
      "outputs": [],
      "source": [
        "PyData.device, PyY.device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e081b20a",
      "metadata": {
        "id": "e081b20a"
      },
      "outputs": [],
      "source": [
        "PyData = PyData.to(device)\n",
        "PyY = PyY.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f35acd1a",
      "metadata": {
        "id": "f35acd1a"
      },
      "outputs": [],
      "source": [
        "PyData.device, PyY.device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b251e85b",
      "metadata": {
        "id": "b251e85b"
      },
      "outputs": [],
      "source": [
        "# PyData_mean = torch.mean(PyData, dim=0)\n",
        "# PyData_centered = PyData - PyData_mean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e7baccd",
      "metadata": {
        "id": "3e7baccd"
      },
      "outputs": [],
      "source": [
        "pca = torch.pca_lowrank(PyData)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c9b0b41",
      "metadata": {
        "id": "6c9b0b41"
      },
      "outputs": [],
      "source": [
        "pca"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0c15bb5",
      "metadata": {
        "id": "b0c15bb5"
      },
      "outputs": [],
      "source": [
        "pca_x = pca[0][:,:5]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "5bNgGS17p8AJ",
      "metadata": {
        "id": "5bNgGS17p8AJ"
      },
      "source": [
        "# Neural Network MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30b5e787",
      "metadata": {
        "id": "30b5e787"
      },
      "outputs": [],
      "source": [
        "X_train = pca_x[:160]\n",
        "X_test = pca_x[-40:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f696b3e1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f696b3e1",
        "outputId": "454a1809-398c-495b-a1d9-69337e6d908b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([40, 5])"
            ]
          },
          "execution_count": 117,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28ea61a1",
      "metadata": {
        "id": "28ea61a1"
      },
      "outputs": [],
      "source": [
        "y_train = PyY[:160]\n",
        "y_test = PyY[-40:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ffa191da",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ffa191da",
        "outputId": "775c54dd-4586-4ec7-f464-ed53bf39f5e0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,\n",
              "        1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0], device='cuda:0')"
            ]
          },
          "execution_count": 119,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "iRV8TibSTl0W",
      "metadata": {
        "id": "iRV8TibSTl0W"
      },
      "outputs": [],
      "source": [
        "from collections import OrderedDict\n",
        "import torch.nn as nn"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "e0b9acef",
      "metadata": {},
      "source": [
        "This is a function that defines a neural network model using the PyTorch library in Python. The model has an input layer with infet input features, a hidden layer with hiddDim neurons, and an output layer with nbClass output classes.\n",
        "\n",
        "The nn.Sequential container is used to sequentially stack the layers of the neural network. In this case, the neural network has two layers - a fully connected linear layer (nn.Linear) and a ReLU activation layer (nn.ReLU). The first layer takes the input and applies a linear transformation to it, producing hiddDim output values. The output of the first layer is then passed through the ReLU activation function, which applies a non-linear transformation to it. The output of the activation function is then passed through another linear layer (nn.Linear) to produce the final output of the model.\n",
        "\n",
        "The function returns the constructed model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zWES6-UwTw79",
      "metadata": {
        "id": "zWES6-UwTw79"
      },
      "outputs": [],
      "source": [
        "def model(infet=5,hiddDim=8,nbClass=2):\n",
        "  model = nn.Sequential(\n",
        "      OrderedDict([\n",
        "          ('hiddenLayer1', nn.Linear(infet,hiddDim)),\n",
        "          ('activation1',nn.ReLU()),\n",
        "          (\"output_layer\", nn.Linear(hiddDim, nbClass))\n",
        "      ])\n",
        "  )\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2Yw6teX0VI2k",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Yw6teX0VI2k",
        "outputId": "133d44a7-b4e1-4924-8a1c-2140155aa73d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (hiddenLayer1): Linear(in_features=5, out_features=8, bias=True)\n",
              "  (activation1): ReLU()\n",
              "  (output_layer): Linear(in_features=8, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 122,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model1 = model()\n",
        "mlp = model1.to(device)\n",
        "mlp"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "41b38316",
      "metadata": {},
      "source": [
        "Input (5 features) -> Hidden Layer (8 neurons) -> ReLU Activation -> Output Layer (2 neurons)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_d9gNc70V3SM",
      "metadata": {
        "id": "_d9gNc70V3SM"
      },
      "outputs": [],
      "source": [
        "# from pyimagesearch import mlp\n",
        "from torch.optim import SGD\n",
        "from sklearn.model_selection import train_test_split\n",
        "# from sklearn.datasets import make_blobs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ozeGv59eVSIu",
      "metadata": {
        "id": "ozeGv59eVSIu"
      },
      "outputs": [],
      "source": [
        "opt = SGD(mlp.parameters(),lr=1e-2)\n",
        "lossFunc = nn.CrossEntropyLoss()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "8a327bb4",
      "metadata": {},
      "source": [
        "opt = SGD(mlp.parameters(), lr=1e-2) creates an instance of the stochastic gradient descent (SGD) optimizer to update the parameters of the mlp model during training. The lr parameter sets the learning rate for the optimizer.\n",
        "lossFunc = nn.CrossEntropyLoss() creates an instance of the cross-entropy loss function, which is commonly used for multi-class classification problems. This loss function computes the negative log-likelihood of the predicted class probabilities for each sample in the batch, given the true class labels. The loss is then used to compute gradients for backpropagation and update the model parameters during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "q6tHT6Yses9v",
      "metadata": {
        "id": "q6tHT6Yses9v"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 300\n",
        "BATCH_SIZE = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kGYfK8jme_3z",
      "metadata": {
        "id": "kGYfK8jme_3z"
      },
      "outputs": [],
      "source": [
        "def next_batch(inputs, targets, batchSize):\n",
        "\t# loop over the dataset\n",
        "\tfor i in range(0, inputs.shape[0], batchSize):\n",
        "\t\t# yield a tuple of the current batched data and labels\n",
        "\t\tyield (inputs[i:i + batchSize], targets[i:i + batchSize])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JBD5zqxlV1vc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JBD5zqxlV1vc",
        "outputId": "68119878-a0d5-43a6-eb9b-d0c989e50d69"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] epoch: 1...\n",
            "epoch: 1 train loss: 0.696 train accuracy: 0.512\n",
            "[INFO] epoch: 2...\n",
            "epoch: 2 train loss: 0.696 train accuracy: 0.512\n",
            "[INFO] epoch: 3...\n",
            "epoch: 3 train loss: 0.696 train accuracy: 0.512\n",
            "[INFO] epoch: 4...\n",
            "epoch: 4 train loss: 0.696 train accuracy: 0.512\n",
            "[INFO] epoch: 5...\n",
            "epoch: 5 train loss: 0.696 train accuracy: 0.512\n",
            "[INFO] epoch: 6...\n",
            "epoch: 6 train loss: 0.696 train accuracy: 0.512\n",
            "[INFO] epoch: 7...\n",
            "epoch: 7 train loss: 0.696 train accuracy: 0.512\n",
            "[INFO] epoch: 8...\n",
            "epoch: 8 train loss: 0.696 train accuracy: 0.512\n",
            "[INFO] epoch: 9...\n",
            "epoch: 9 train loss: 0.696 train accuracy: 0.512\n",
            "[INFO] epoch: 10...\n",
            "epoch: 10 train loss: 0.695 train accuracy: 0.512\n",
            "[INFO] epoch: 11...\n",
            "epoch: 11 train loss: 0.695 train accuracy: 0.512\n",
            "[INFO] epoch: 12...\n",
            "epoch: 12 train loss: 0.695 train accuracy: 0.512\n",
            "[INFO] epoch: 13...\n",
            "epoch: 13 train loss: 0.695 train accuracy: 0.512\n",
            "[INFO] epoch: 14...\n",
            "epoch: 14 train loss: 0.695 train accuracy: 0.512\n",
            "[INFO] epoch: 15...\n",
            "epoch: 15 train loss: 0.695 train accuracy: 0.512\n",
            "[INFO] epoch: 16...\n",
            "epoch: 16 train loss: 0.695 train accuracy: 0.512\n",
            "[INFO] epoch: 17...\n",
            "epoch: 17 train loss: 0.695 train accuracy: 0.512\n",
            "[INFO] epoch: 18...\n",
            "epoch: 18 train loss: 0.695 train accuracy: 0.512\n",
            "[INFO] epoch: 19...\n",
            "epoch: 19 train loss: 0.695 train accuracy: 0.512\n",
            "[INFO] epoch: 20...\n",
            "epoch: 20 train loss: 0.695 train accuracy: 0.512\n",
            "[INFO] epoch: 21...\n",
            "epoch: 21 train loss: 0.694 train accuracy: 0.512\n",
            "[INFO] epoch: 22...\n",
            "epoch: 22 train loss: 0.694 train accuracy: 0.512\n",
            "[INFO] epoch: 23...\n",
            "epoch: 23 train loss: 0.694 train accuracy: 0.512\n",
            "[INFO] epoch: 24...\n",
            "epoch: 24 train loss: 0.694 train accuracy: 0.512\n",
            "[INFO] epoch: 25...\n",
            "epoch: 25 train loss: 0.694 train accuracy: 0.512\n",
            "[INFO] epoch: 26...\n",
            "epoch: 26 train loss: 0.694 train accuracy: 0.512\n",
            "[INFO] epoch: 27...\n",
            "epoch: 27 train loss: 0.694 train accuracy: 0.512\n",
            "[INFO] epoch: 28...\n",
            "epoch: 28 train loss: 0.694 train accuracy: 0.512\n",
            "[INFO] epoch: 29...\n",
            "epoch: 29 train loss: 0.694 train accuracy: 0.512\n",
            "[INFO] epoch: 30...\n",
            "epoch: 30 train loss: 0.694 train accuracy: 0.512\n",
            "[INFO] epoch: 31...\n",
            "epoch: 31 train loss: 0.693 train accuracy: 0.512\n",
            "[INFO] epoch: 32...\n",
            "epoch: 32 train loss: 0.693 train accuracy: 0.512\n",
            "[INFO] epoch: 33...\n",
            "epoch: 33 train loss: 0.693 train accuracy: 0.512\n",
            "[INFO] epoch: 34...\n",
            "epoch: 34 train loss: 0.693 train accuracy: 0.512\n",
            "[INFO] epoch: 35...\n",
            "epoch: 35 train loss: 0.693 train accuracy: 0.512\n",
            "[INFO] epoch: 36...\n",
            "epoch: 36 train loss: 0.693 train accuracy: 0.512\n",
            "[INFO] epoch: 37...\n",
            "epoch: 37 train loss: 0.693 train accuracy: 0.512\n",
            "[INFO] epoch: 38...\n",
            "epoch: 38 train loss: 0.692 train accuracy: 0.512\n",
            "[INFO] epoch: 39...\n",
            "epoch: 39 train loss: 0.692 train accuracy: 0.512\n",
            "[INFO] epoch: 40...\n",
            "epoch: 40 train loss: 0.692 train accuracy: 0.512\n",
            "[INFO] epoch: 41...\n",
            "epoch: 41 train loss: 0.692 train accuracy: 0.512\n",
            "[INFO] epoch: 42...\n",
            "epoch: 42 train loss: 0.692 train accuracy: 0.512\n",
            "[INFO] epoch: 43...\n",
            "epoch: 43 train loss: 0.692 train accuracy: 0.512\n",
            "[INFO] epoch: 44...\n",
            "epoch: 44 train loss: 0.692 train accuracy: 0.512\n",
            "[INFO] epoch: 45...\n",
            "epoch: 45 train loss: 0.691 train accuracy: 0.512\n",
            "[INFO] epoch: 46...\n",
            "epoch: 46 train loss: 0.691 train accuracy: 0.512\n",
            "[INFO] epoch: 47...\n",
            "epoch: 47 train loss: 0.691 train accuracy: 0.512\n",
            "[INFO] epoch: 48...\n",
            "epoch: 48 train loss: 0.691 train accuracy: 0.512\n",
            "[INFO] epoch: 49...\n",
            "epoch: 49 train loss: 0.691 train accuracy: 0.512\n",
            "[INFO] epoch: 50...\n",
            "epoch: 50 train loss: 0.691 train accuracy: 0.512\n",
            "[INFO] epoch: 51...\n",
            "epoch: 51 train loss: 0.690 train accuracy: 0.512\n",
            "[INFO] epoch: 52...\n",
            "epoch: 52 train loss: 0.690 train accuracy: 0.512\n",
            "[INFO] epoch: 53...\n",
            "epoch: 53 train loss: 0.690 train accuracy: 0.512\n",
            "[INFO] epoch: 54...\n",
            "epoch: 54 train loss: 0.690 train accuracy: 0.512\n",
            "[INFO] epoch: 55...\n",
            "epoch: 55 train loss: 0.690 train accuracy: 0.512\n",
            "[INFO] epoch: 56...\n",
            "epoch: 56 train loss: 0.689 train accuracy: 0.512\n",
            "[INFO] epoch: 57...\n",
            "epoch: 57 train loss: 0.689 train accuracy: 0.512\n",
            "[INFO] epoch: 58...\n",
            "epoch: 58 train loss: 0.689 train accuracy: 0.512\n",
            "[INFO] epoch: 59...\n",
            "epoch: 59 train loss: 0.689 train accuracy: 0.512\n",
            "[INFO] epoch: 60...\n",
            "epoch: 60 train loss: 0.689 train accuracy: 0.512\n",
            "[INFO] epoch: 61...\n",
            "epoch: 61 train loss: 0.688 train accuracy: 0.512\n",
            "[INFO] epoch: 62...\n",
            "epoch: 62 train loss: 0.688 train accuracy: 0.512\n",
            "[INFO] epoch: 63...\n",
            "epoch: 63 train loss: 0.688 train accuracy: 0.512\n",
            "[INFO] epoch: 64...\n",
            "epoch: 64 train loss: 0.688 train accuracy: 0.512\n",
            "[INFO] epoch: 65...\n",
            "epoch: 65 train loss: 0.688 train accuracy: 0.512\n",
            "[INFO] epoch: 66...\n",
            "epoch: 66 train loss: 0.687 train accuracy: 0.512\n",
            "[INFO] epoch: 67...\n",
            "epoch: 67 train loss: 0.687 train accuracy: 0.512\n",
            "[INFO] epoch: 68...\n",
            "epoch: 68 train loss: 0.687 train accuracy: 0.512\n",
            "[INFO] epoch: 69...\n",
            "epoch: 69 train loss: 0.687 train accuracy: 0.512\n",
            "[INFO] epoch: 70...\n",
            "epoch: 70 train loss: 0.686 train accuracy: 0.512\n",
            "[INFO] epoch: 71...\n",
            "epoch: 71 train loss: 0.686 train accuracy: 0.512\n",
            "[INFO] epoch: 72...\n",
            "epoch: 72 train loss: 0.686 train accuracy: 0.512\n",
            "[INFO] epoch: 73...\n",
            "epoch: 73 train loss: 0.686 train accuracy: 0.512\n",
            "[INFO] epoch: 74...\n",
            "epoch: 74 train loss: 0.686 train accuracy: 0.512\n",
            "[INFO] epoch: 75...\n",
            "epoch: 75 train loss: 0.685 train accuracy: 0.512\n",
            "[INFO] epoch: 76...\n",
            "epoch: 76 train loss: 0.685 train accuracy: 0.512\n",
            "[INFO] epoch: 77...\n",
            "epoch: 77 train loss: 0.685 train accuracy: 0.512\n",
            "[INFO] epoch: 78...\n",
            "epoch: 78 train loss: 0.685 train accuracy: 0.512\n",
            "[INFO] epoch: 79...\n",
            "epoch: 79 train loss: 0.684 train accuracy: 0.512\n",
            "[INFO] epoch: 80...\n",
            "epoch: 80 train loss: 0.684 train accuracy: 0.512\n",
            "[INFO] epoch: 81...\n",
            "epoch: 81 train loss: 0.684 train accuracy: 0.512\n",
            "[INFO] epoch: 82...\n",
            "epoch: 82 train loss: 0.684 train accuracy: 0.512\n",
            "[INFO] epoch: 83...\n",
            "epoch: 83 train loss: 0.683 train accuracy: 0.512\n",
            "[INFO] epoch: 84...\n",
            "epoch: 84 train loss: 0.683 train accuracy: 0.512\n",
            "[INFO] epoch: 85...\n",
            "epoch: 85 train loss: 0.683 train accuracy: 0.512\n",
            "[INFO] epoch: 86...\n",
            "epoch: 86 train loss: 0.683 train accuracy: 0.512\n",
            "[INFO] epoch: 87...\n",
            "epoch: 87 train loss: 0.682 train accuracy: 0.512\n",
            "[INFO] epoch: 88...\n",
            "epoch: 88 train loss: 0.682 train accuracy: 0.512\n",
            "[INFO] epoch: 89...\n",
            "epoch: 89 train loss: 0.682 train accuracy: 0.512\n",
            "[INFO] epoch: 90...\n",
            "epoch: 90 train loss: 0.682 train accuracy: 0.512\n",
            "[INFO] epoch: 91...\n",
            "epoch: 91 train loss: 0.681 train accuracy: 0.512\n",
            "[INFO] epoch: 92...\n",
            "epoch: 92 train loss: 0.681 train accuracy: 0.512\n",
            "[INFO] epoch: 93...\n",
            "epoch: 93 train loss: 0.681 train accuracy: 0.512\n",
            "[INFO] epoch: 94...\n",
            "epoch: 94 train loss: 0.681 train accuracy: 0.519\n",
            "[INFO] epoch: 95...\n",
            "epoch: 95 train loss: 0.680 train accuracy: 0.525\n",
            "[INFO] epoch: 96...\n",
            "epoch: 96 train loss: 0.680 train accuracy: 0.531\n",
            "[INFO] epoch: 97...\n",
            "epoch: 97 train loss: 0.680 train accuracy: 0.537\n",
            "[INFO] epoch: 98...\n",
            "epoch: 98 train loss: 0.680 train accuracy: 0.544\n",
            "[INFO] epoch: 99...\n",
            "epoch: 99 train loss: 0.679 train accuracy: 0.544\n",
            "[INFO] epoch: 100...\n",
            "epoch: 100 train loss: 0.679 train accuracy: 0.544\n",
            "[INFO] epoch: 101...\n",
            "epoch: 101 train loss: 0.679 train accuracy: 0.544\n",
            "[INFO] epoch: 102...\n",
            "epoch: 102 train loss: 0.679 train accuracy: 0.550\n",
            "[INFO] epoch: 103...\n",
            "epoch: 103 train loss: 0.678 train accuracy: 0.550\n",
            "[INFO] epoch: 104...\n",
            "epoch: 104 train loss: 0.678 train accuracy: 0.550\n",
            "[INFO] epoch: 105...\n",
            "epoch: 105 train loss: 0.678 train accuracy: 0.556\n",
            "[INFO] epoch: 106...\n",
            "epoch: 106 train loss: 0.677 train accuracy: 0.562\n",
            "[INFO] epoch: 107...\n",
            "epoch: 107 train loss: 0.677 train accuracy: 0.569\n",
            "[INFO] epoch: 108...\n",
            "epoch: 108 train loss: 0.677 train accuracy: 0.562\n",
            "[INFO] epoch: 109...\n",
            "epoch: 109 train loss: 0.676 train accuracy: 0.569\n",
            "[INFO] epoch: 110...\n",
            "epoch: 110 train loss: 0.676 train accuracy: 0.569\n",
            "[INFO] epoch: 111...\n",
            "epoch: 111 train loss: 0.676 train accuracy: 0.569\n",
            "[INFO] epoch: 112...\n",
            "epoch: 112 train loss: 0.676 train accuracy: 0.569\n",
            "[INFO] epoch: 113...\n",
            "epoch: 113 train loss: 0.675 train accuracy: 0.581\n",
            "[INFO] epoch: 114...\n",
            "epoch: 114 train loss: 0.675 train accuracy: 0.600\n",
            "[INFO] epoch: 115...\n",
            "epoch: 115 train loss: 0.675 train accuracy: 0.606\n",
            "[INFO] epoch: 116...\n",
            "epoch: 116 train loss: 0.674 train accuracy: 0.619\n",
            "[INFO] epoch: 117...\n",
            "epoch: 117 train loss: 0.674 train accuracy: 0.625\n",
            "[INFO] epoch: 118...\n",
            "epoch: 118 train loss: 0.674 train accuracy: 0.625\n",
            "[INFO] epoch: 119...\n",
            "epoch: 119 train loss: 0.673 train accuracy: 0.631\n",
            "[INFO] epoch: 120...\n",
            "epoch: 120 train loss: 0.673 train accuracy: 0.631\n",
            "[INFO] epoch: 121...\n",
            "epoch: 121 train loss: 0.673 train accuracy: 0.631\n",
            "[INFO] epoch: 122...\n",
            "epoch: 122 train loss: 0.672 train accuracy: 0.631\n",
            "[INFO] epoch: 123...\n",
            "epoch: 123 train loss: 0.672 train accuracy: 0.637\n",
            "[INFO] epoch: 124...\n",
            "epoch: 124 train loss: 0.672 train accuracy: 0.637\n",
            "[INFO] epoch: 125...\n",
            "epoch: 125 train loss: 0.671 train accuracy: 0.644\n",
            "[INFO] epoch: 126...\n",
            "epoch: 126 train loss: 0.671 train accuracy: 0.644\n",
            "[INFO] epoch: 127...\n",
            "epoch: 127 train loss: 0.670 train accuracy: 0.656\n",
            "[INFO] epoch: 128...\n",
            "epoch: 128 train loss: 0.670 train accuracy: 0.662\n",
            "[INFO] epoch: 129...\n",
            "epoch: 129 train loss: 0.670 train accuracy: 0.662\n",
            "[INFO] epoch: 130...\n",
            "epoch: 130 train loss: 0.669 train accuracy: 0.675\n",
            "[INFO] epoch: 131...\n",
            "epoch: 131 train loss: 0.669 train accuracy: 0.681\n",
            "[INFO] epoch: 132...\n",
            "epoch: 132 train loss: 0.669 train accuracy: 0.688\n",
            "[INFO] epoch: 133...\n",
            "epoch: 133 train loss: 0.668 train accuracy: 0.688\n",
            "[INFO] epoch: 134...\n",
            "epoch: 134 train loss: 0.668 train accuracy: 0.694\n",
            "[INFO] epoch: 135...\n",
            "epoch: 135 train loss: 0.667 train accuracy: 0.700\n",
            "[INFO] epoch: 136...\n",
            "epoch: 136 train loss: 0.667 train accuracy: 0.700\n",
            "[INFO] epoch: 137...\n",
            "epoch: 137 train loss: 0.667 train accuracy: 0.700\n",
            "[INFO] epoch: 138...\n",
            "epoch: 138 train loss: 0.666 train accuracy: 0.700\n",
            "[INFO] epoch: 139...\n",
            "epoch: 139 train loss: 0.666 train accuracy: 0.700\n",
            "[INFO] epoch: 140...\n",
            "epoch: 140 train loss: 0.665 train accuracy: 0.700\n",
            "[INFO] epoch: 141...\n",
            "epoch: 141 train loss: 0.665 train accuracy: 0.700\n",
            "[INFO] epoch: 142...\n",
            "epoch: 142 train loss: 0.664 train accuracy: 0.700\n",
            "[INFO] epoch: 143...\n",
            "epoch: 143 train loss: 0.664 train accuracy: 0.706\n",
            "[INFO] epoch: 144...\n",
            "epoch: 144 train loss: 0.664 train accuracy: 0.706\n",
            "[INFO] epoch: 145...\n",
            "epoch: 145 train loss: 0.663 train accuracy: 0.713\n",
            "[INFO] epoch: 146...\n",
            "epoch: 146 train loss: 0.663 train accuracy: 0.713\n",
            "[INFO] epoch: 147...\n",
            "epoch: 147 train loss: 0.662 train accuracy: 0.713\n",
            "[INFO] epoch: 148...\n",
            "epoch: 148 train loss: 0.662 train accuracy: 0.713\n",
            "[INFO] epoch: 149...\n",
            "epoch: 149 train loss: 0.661 train accuracy: 0.713\n",
            "[INFO] epoch: 150...\n",
            "epoch: 150 train loss: 0.661 train accuracy: 0.719\n",
            "[INFO] epoch: 151...\n",
            "epoch: 151 train loss: 0.660 train accuracy: 0.719\n",
            "[INFO] epoch: 152...\n",
            "epoch: 152 train loss: 0.660 train accuracy: 0.719\n",
            "[INFO] epoch: 153...\n",
            "epoch: 153 train loss: 0.659 train accuracy: 0.719\n",
            "[INFO] epoch: 154...\n",
            "epoch: 154 train loss: 0.659 train accuracy: 0.731\n",
            "[INFO] epoch: 155...\n",
            "epoch: 155 train loss: 0.658 train accuracy: 0.731\n",
            "[INFO] epoch: 156...\n",
            "epoch: 156 train loss: 0.658 train accuracy: 0.738\n",
            "[INFO] epoch: 157...\n",
            "epoch: 157 train loss: 0.657 train accuracy: 0.744\n",
            "[INFO] epoch: 158...\n",
            "epoch: 158 train loss: 0.657 train accuracy: 0.756\n",
            "[INFO] epoch: 159...\n",
            "epoch: 159 train loss: 0.656 train accuracy: 0.756\n",
            "[INFO] epoch: 160...\n",
            "epoch: 160 train loss: 0.656 train accuracy: 0.756\n",
            "[INFO] epoch: 161...\n",
            "epoch: 161 train loss: 0.655 train accuracy: 0.756\n",
            "[INFO] epoch: 162...\n",
            "epoch: 162 train loss: 0.655 train accuracy: 0.756\n",
            "[INFO] epoch: 163...\n",
            "epoch: 163 train loss: 0.654 train accuracy: 0.769\n",
            "[INFO] epoch: 164...\n",
            "epoch: 164 train loss: 0.653 train accuracy: 0.775\n",
            "[INFO] epoch: 165...\n",
            "epoch: 165 train loss: 0.653 train accuracy: 0.775\n",
            "[INFO] epoch: 166...\n",
            "epoch: 166 train loss: 0.652 train accuracy: 0.781\n",
            "[INFO] epoch: 167...\n",
            "epoch: 167 train loss: 0.652 train accuracy: 0.781\n",
            "[INFO] epoch: 168...\n",
            "epoch: 168 train loss: 0.651 train accuracy: 0.781\n",
            "[INFO] epoch: 169...\n",
            "epoch: 169 train loss: 0.650 train accuracy: 0.787\n",
            "[INFO] epoch: 170...\n",
            "epoch: 170 train loss: 0.650 train accuracy: 0.787\n",
            "[INFO] epoch: 171...\n",
            "epoch: 171 train loss: 0.649 train accuracy: 0.787\n",
            "[INFO] epoch: 172...\n",
            "epoch: 172 train loss: 0.649 train accuracy: 0.787\n",
            "[INFO] epoch: 173...\n",
            "epoch: 173 train loss: 0.648 train accuracy: 0.794\n",
            "[INFO] epoch: 174...\n",
            "epoch: 174 train loss: 0.647 train accuracy: 0.794\n",
            "[INFO] epoch: 175...\n",
            "epoch: 175 train loss: 0.647 train accuracy: 0.794\n",
            "[INFO] epoch: 176...\n",
            "epoch: 176 train loss: 0.646 train accuracy: 0.794\n",
            "[INFO] epoch: 177...\n",
            "epoch: 177 train loss: 0.645 train accuracy: 0.800\n",
            "[INFO] epoch: 178...\n",
            "epoch: 178 train loss: 0.645 train accuracy: 0.800\n",
            "[INFO] epoch: 179...\n",
            "epoch: 179 train loss: 0.644 train accuracy: 0.800\n",
            "[INFO] epoch: 180...\n",
            "epoch: 180 train loss: 0.643 train accuracy: 0.800\n",
            "[INFO] epoch: 181...\n",
            "epoch: 181 train loss: 0.643 train accuracy: 0.800\n",
            "[INFO] epoch: 182...\n",
            "epoch: 182 train loss: 0.642 train accuracy: 0.806\n",
            "[INFO] epoch: 183...\n",
            "epoch: 183 train loss: 0.641 train accuracy: 0.806\n",
            "[INFO] epoch: 184...\n",
            "epoch: 184 train loss: 0.640 train accuracy: 0.812\n",
            "[INFO] epoch: 185...\n",
            "epoch: 185 train loss: 0.640 train accuracy: 0.812\n",
            "[INFO] epoch: 186...\n",
            "epoch: 186 train loss: 0.639 train accuracy: 0.812\n",
            "[INFO] epoch: 187...\n",
            "epoch: 187 train loss: 0.638 train accuracy: 0.819\n",
            "[INFO] epoch: 188...\n",
            "epoch: 188 train loss: 0.638 train accuracy: 0.819\n",
            "[INFO] epoch: 189...\n",
            "epoch: 189 train loss: 0.637 train accuracy: 0.819\n",
            "[INFO] epoch: 190...\n",
            "epoch: 190 train loss: 0.636 train accuracy: 0.819\n",
            "[INFO] epoch: 191...\n",
            "epoch: 191 train loss: 0.635 train accuracy: 0.819\n",
            "[INFO] epoch: 192...\n",
            "epoch: 192 train loss: 0.634 train accuracy: 0.819\n",
            "[INFO] epoch: 193...\n",
            "epoch: 193 train loss: 0.634 train accuracy: 0.825\n",
            "[INFO] epoch: 194...\n",
            "epoch: 194 train loss: 0.633 train accuracy: 0.831\n",
            "[INFO] epoch: 195...\n",
            "epoch: 195 train loss: 0.632 train accuracy: 0.838\n",
            "[INFO] epoch: 196...\n",
            "epoch: 196 train loss: 0.631 train accuracy: 0.838\n",
            "[INFO] epoch: 197...\n",
            "epoch: 197 train loss: 0.630 train accuracy: 0.838\n",
            "[INFO] epoch: 198...\n",
            "epoch: 198 train loss: 0.630 train accuracy: 0.838\n",
            "[INFO] epoch: 199...\n",
            "epoch: 199 train loss: 0.629 train accuracy: 0.844\n",
            "[INFO] epoch: 200...\n",
            "epoch: 200 train loss: 0.628 train accuracy: 0.844\n",
            "[INFO] epoch: 201...\n",
            "epoch: 201 train loss: 0.627 train accuracy: 0.844\n",
            "[INFO] epoch: 202...\n",
            "epoch: 202 train loss: 0.626 train accuracy: 0.844\n",
            "[INFO] epoch: 203...\n",
            "epoch: 203 train loss: 0.625 train accuracy: 0.844\n",
            "[INFO] epoch: 204...\n",
            "epoch: 204 train loss: 0.624 train accuracy: 0.844\n",
            "[INFO] epoch: 205...\n",
            "epoch: 205 train loss: 0.623 train accuracy: 0.850\n",
            "[INFO] epoch: 206...\n",
            "epoch: 206 train loss: 0.622 train accuracy: 0.863\n",
            "[INFO] epoch: 207...\n",
            "epoch: 207 train loss: 0.622 train accuracy: 0.863\n",
            "[INFO] epoch: 208...\n",
            "epoch: 208 train loss: 0.621 train accuracy: 0.863\n",
            "[INFO] epoch: 209...\n",
            "epoch: 209 train loss: 0.620 train accuracy: 0.863\n",
            "[INFO] epoch: 210...\n",
            "epoch: 210 train loss: 0.619 train accuracy: 0.863\n",
            "[INFO] epoch: 211...\n",
            "epoch: 211 train loss: 0.618 train accuracy: 0.863\n",
            "[INFO] epoch: 212...\n",
            "epoch: 212 train loss: 0.617 train accuracy: 0.863\n",
            "[INFO] epoch: 213...\n",
            "epoch: 213 train loss: 0.616 train accuracy: 0.863\n",
            "[INFO] epoch: 214...\n",
            "epoch: 214 train loss: 0.615 train accuracy: 0.863\n",
            "[INFO] epoch: 215...\n",
            "epoch: 215 train loss: 0.614 train accuracy: 0.863\n",
            "[INFO] epoch: 216...\n",
            "epoch: 216 train loss: 0.613 train accuracy: 0.863\n",
            "[INFO] epoch: 217...\n",
            "epoch: 217 train loss: 0.612 train accuracy: 0.863\n",
            "[INFO] epoch: 218...\n",
            "epoch: 218 train loss: 0.611 train accuracy: 0.863\n",
            "[INFO] epoch: 219...\n",
            "epoch: 219 train loss: 0.610 train accuracy: 0.863\n",
            "[INFO] epoch: 220...\n",
            "epoch: 220 train loss: 0.609 train accuracy: 0.863\n",
            "[INFO] epoch: 221...\n",
            "epoch: 221 train loss: 0.607 train accuracy: 0.863\n",
            "[INFO] epoch: 222...\n",
            "epoch: 222 train loss: 0.606 train accuracy: 0.869\n",
            "[INFO] epoch: 223...\n",
            "epoch: 223 train loss: 0.605 train accuracy: 0.875\n",
            "[INFO] epoch: 224...\n",
            "epoch: 224 train loss: 0.604 train accuracy: 0.881\n",
            "[INFO] epoch: 225...\n",
            "epoch: 225 train loss: 0.603 train accuracy: 0.881\n",
            "[INFO] epoch: 226...\n",
            "epoch: 226 train loss: 0.602 train accuracy: 0.887\n",
            "[INFO] epoch: 227...\n",
            "epoch: 227 train loss: 0.601 train accuracy: 0.894\n",
            "[INFO] epoch: 228...\n",
            "epoch: 228 train loss: 0.600 train accuracy: 0.894\n",
            "[INFO] epoch: 229...\n",
            "epoch: 229 train loss: 0.598 train accuracy: 0.906\n",
            "[INFO] epoch: 230...\n",
            "epoch: 230 train loss: 0.597 train accuracy: 0.906\n",
            "[INFO] epoch: 231...\n",
            "epoch: 231 train loss: 0.596 train accuracy: 0.906\n",
            "[INFO] epoch: 232...\n",
            "epoch: 232 train loss: 0.595 train accuracy: 0.912\n",
            "[INFO] epoch: 233...\n",
            "epoch: 233 train loss: 0.594 train accuracy: 0.912\n",
            "[INFO] epoch: 234...\n",
            "epoch: 234 train loss: 0.592 train accuracy: 0.919\n",
            "[INFO] epoch: 235...\n",
            "epoch: 235 train loss: 0.591 train accuracy: 0.919\n",
            "[INFO] epoch: 236...\n",
            "epoch: 236 train loss: 0.590 train accuracy: 0.925\n",
            "[INFO] epoch: 237...\n",
            "epoch: 237 train loss: 0.589 train accuracy: 0.931\n",
            "[INFO] epoch: 238...\n",
            "epoch: 238 train loss: 0.587 train accuracy: 0.931\n",
            "[INFO] epoch: 239...\n",
            "epoch: 239 train loss: 0.586 train accuracy: 0.931\n",
            "[INFO] epoch: 240...\n",
            "epoch: 240 train loss: 0.585 train accuracy: 0.931\n",
            "[INFO] epoch: 241...\n",
            "epoch: 241 train loss: 0.584 train accuracy: 0.931\n",
            "[INFO] epoch: 242...\n",
            "epoch: 242 train loss: 0.582 train accuracy: 0.931\n",
            "[INFO] epoch: 243...\n",
            "epoch: 243 train loss: 0.581 train accuracy: 0.931\n",
            "[INFO] epoch: 244...\n",
            "epoch: 244 train loss: 0.580 train accuracy: 0.938\n",
            "[INFO] epoch: 245...\n",
            "epoch: 245 train loss: 0.578 train accuracy: 0.944\n",
            "[INFO] epoch: 246...\n",
            "epoch: 246 train loss: 0.577 train accuracy: 0.944\n",
            "[INFO] epoch: 247...\n",
            "epoch: 247 train loss: 0.576 train accuracy: 0.944\n",
            "[INFO] epoch: 248...\n",
            "epoch: 248 train loss: 0.574 train accuracy: 0.944\n",
            "[INFO] epoch: 249...\n",
            "epoch: 249 train loss: 0.573 train accuracy: 0.950\n",
            "[INFO] epoch: 250...\n",
            "epoch: 250 train loss: 0.571 train accuracy: 0.950\n",
            "[INFO] epoch: 251...\n",
            "epoch: 251 train loss: 0.570 train accuracy: 0.956\n",
            "[INFO] epoch: 252...\n",
            "epoch: 252 train loss: 0.568 train accuracy: 0.956\n",
            "[INFO] epoch: 253...\n",
            "epoch: 253 train loss: 0.567 train accuracy: 0.956\n",
            "[INFO] epoch: 254...\n",
            "epoch: 254 train loss: 0.566 train accuracy: 0.963\n",
            "[INFO] epoch: 255...\n",
            "epoch: 255 train loss: 0.564 train accuracy: 0.963\n",
            "[INFO] epoch: 256...\n",
            "epoch: 256 train loss: 0.563 train accuracy: 0.963\n",
            "[INFO] epoch: 257...\n",
            "epoch: 257 train loss: 0.561 train accuracy: 0.963\n",
            "[INFO] epoch: 258...\n",
            "epoch: 258 train loss: 0.560 train accuracy: 0.963\n",
            "[INFO] epoch: 259...\n",
            "epoch: 259 train loss: 0.558 train accuracy: 0.969\n",
            "[INFO] epoch: 260...\n",
            "epoch: 260 train loss: 0.557 train accuracy: 0.969\n",
            "[INFO] epoch: 261...\n",
            "epoch: 261 train loss: 0.555 train accuracy: 0.969\n",
            "[INFO] epoch: 262...\n",
            "epoch: 262 train loss: 0.554 train accuracy: 0.969\n",
            "[INFO] epoch: 263...\n",
            "epoch: 263 train loss: 0.552 train accuracy: 0.969\n",
            "[INFO] epoch: 264...\n",
            "epoch: 264 train loss: 0.550 train accuracy: 0.969\n",
            "[INFO] epoch: 265...\n",
            "epoch: 265 train loss: 0.549 train accuracy: 0.969\n",
            "[INFO] epoch: 266...\n",
            "epoch: 266 train loss: 0.547 train accuracy: 0.969\n",
            "[INFO] epoch: 267...\n",
            "epoch: 267 train loss: 0.546 train accuracy: 0.969\n",
            "[INFO] epoch: 268...\n",
            "epoch: 268 train loss: 0.544 train accuracy: 0.969\n",
            "[INFO] epoch: 269...\n",
            "epoch: 269 train loss: 0.542 train accuracy: 0.969\n",
            "[INFO] epoch: 270...\n",
            "epoch: 270 train loss: 0.541 train accuracy: 0.969\n",
            "[INFO] epoch: 271...\n",
            "epoch: 271 train loss: 0.539 train accuracy: 0.969\n",
            "[INFO] epoch: 272...\n",
            "epoch: 272 train loss: 0.538 train accuracy: 0.969\n",
            "[INFO] epoch: 273...\n",
            "epoch: 273 train loss: 0.536 train accuracy: 0.969\n",
            "[INFO] epoch: 274...\n",
            "epoch: 274 train loss: 0.534 train accuracy: 0.969\n",
            "[INFO] epoch: 275...\n",
            "epoch: 275 train loss: 0.532 train accuracy: 0.969\n",
            "[INFO] epoch: 276...\n",
            "epoch: 276 train loss: 0.531 train accuracy: 0.969\n",
            "[INFO] epoch: 277...\n",
            "epoch: 277 train loss: 0.529 train accuracy: 0.969\n",
            "[INFO] epoch: 278...\n",
            "epoch: 278 train loss: 0.527 train accuracy: 0.969\n",
            "[INFO] epoch: 279...\n",
            "epoch: 279 train loss: 0.526 train accuracy: 0.969\n",
            "[INFO] epoch: 280...\n",
            "epoch: 280 train loss: 0.524 train accuracy: 0.975\n",
            "[INFO] epoch: 281...\n",
            "epoch: 281 train loss: 0.522 train accuracy: 0.975\n",
            "[INFO] epoch: 282...\n",
            "epoch: 282 train loss: 0.520 train accuracy: 0.975\n",
            "[INFO] epoch: 283...\n",
            "epoch: 283 train loss: 0.519 train accuracy: 0.975\n",
            "[INFO] epoch: 284...\n",
            "epoch: 284 train loss: 0.517 train accuracy: 0.975\n",
            "[INFO] epoch: 285...\n",
            "epoch: 285 train loss: 0.515 train accuracy: 0.975\n",
            "[INFO] epoch: 286...\n",
            "epoch: 286 train loss: 0.513 train accuracy: 0.975\n",
            "[INFO] epoch: 287...\n",
            "epoch: 287 train loss: 0.511 train accuracy: 0.975\n",
            "[INFO] epoch: 288...\n",
            "epoch: 288 train loss: 0.510 train accuracy: 0.975\n",
            "[INFO] epoch: 289...\n",
            "epoch: 289 train loss: 0.508 train accuracy: 0.981\n",
            "[INFO] epoch: 290...\n",
            "epoch: 290 train loss: 0.506 train accuracy: 0.981\n",
            "[INFO] epoch: 291...\n",
            "epoch: 291 train loss: 0.504 train accuracy: 0.981\n",
            "[INFO] epoch: 292...\n",
            "epoch: 292 train loss: 0.502 train accuracy: 0.981\n",
            "[INFO] epoch: 293...\n",
            "epoch: 293 train loss: 0.500 train accuracy: 0.981\n",
            "[INFO] epoch: 294...\n",
            "epoch: 294 train loss: 0.498 train accuracy: 0.981\n",
            "[INFO] epoch: 295...\n",
            "epoch: 295 train loss: 0.497 train accuracy: 0.981\n",
            "[INFO] epoch: 296...\n",
            "epoch: 296 train loss: 0.495 train accuracy: 0.981\n",
            "[INFO] epoch: 297...\n",
            "epoch: 297 train loss: 0.493 train accuracy: 0.981\n",
            "[INFO] epoch: 298...\n",
            "epoch: 298 train loss: 0.491 train accuracy: 0.988\n",
            "[INFO] epoch: 299...\n",
            "epoch: 299 train loss: 0.489 train accuracy: 0.988\n",
            "[INFO] epoch: 300...\n",
            "epoch: 300 train loss: 0.487 train accuracy: 0.988\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(0, EPOCHS):\n",
        "\t# initialize tracker variables and set our model to trainable\n",
        "\tprint(\"[INFO] epoch: {}...\".format(epoch + 1))\n",
        "\ttrainLoss = 0\n",
        "\ttrainAcc = 0\n",
        "\tsamples = 0\n",
        "\tmlp.train()\n",
        "\t# loop over the current batch of data\n",
        "\tfor (batchX, batchY) in next_batch(X_train, y_train, BATCH_SIZE):\n",
        "\t\t# flash data to the current device, run it through our\n",
        "\t\t# model, and calculate loss\n",
        "\t\t(batchX, batchY) = (batchX.to(device), batchY.to(device))\n",
        "\t\tpredictions = mlp(batchX)\n",
        "\t\tloss = lossFunc(predictions, batchY.long())\n",
        "\t\t# zero the gradients accumulated from the previous steps,\n",
        "\t\t# perform backpropagation, and update model parameters\n",
        "\t\topt.zero_grad()\n",
        "\t\tloss.backward()\n",
        "\t\topt.step()\n",
        "\t\t# update training loss, accuracy, and the number of samples\n",
        "\t\t# visited\n",
        "\t\ttrainLoss += loss.item() * batchY.size(0)\n",
        "\t\ttrainAcc += (predictions.max(1)[1] == batchY).sum().item()\n",
        "\t\tsamples += batchY.size(0)\n",
        "\t# display model progress on the current training batch\n",
        "\ttrainTemplate = \"epoch: {} train loss: {:.3f} train accuracy: {:.3f}\"\n",
        "\tprint(trainTemplate.format(epoch + 1, (trainLoss / samples),\n",
        "\t\t(trainAcc / samples)))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "367c1c9e",
      "metadata": {},
      "source": [
        "This code block represents the training loop for a PyTorch MLP (Multi-Layer Perceptron) model. Here's a breakdown of what's happening:\n",
        "\n",
        "- The loop iterates over a set number of epochs.\n",
        "- At the start of each epoch, training loss, accuracy, and the number of samples are initialized to zero.\n",
        "- The model is set to \"trainable\" mode using `mlp.train()`.\n",
        "- The data is processed in batches using `next_batch()` function which returns a generator that yields `BATCH_SIZE` samples of input data and their corresponding labels.\n",
        "- Each batch of data is loaded to the GPU (if available) and passed through the model to make predictions using `mlp(batchX)`.\n",
        "- The predictions are compared against the actual labels using the `nn.CrossEntropyLoss()` function which calculates the loss.\n",
        "- The gradients are zeroed out using `opt.zero_grad()` to prevent the gradients from the previous batch from accumulating.\n",
        "- The gradients are backpropagated using `loss.backward()` to compute the gradients of the loss with respect to the model parameters.\n",
        "- The optimizer is used to update the model parameters using `opt.step()`.\n",
        "- Training loss, accuracy, and number of samples are updated.\n",
        "- The training progress is printed at the end of each epoch.\n",
        "\n",
        "The purpose of this code block is to update the MLP model's weights in such a way that the loss is minimized over the training set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KakebkiVfOmU",
      "metadata": {
        "id": "KakebkiVfOmU"
      },
      "outputs": [],
      "source": [
        "pred = mlp(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "w7MdPtBcUtS4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w7MdPtBcUtS4",
        "outputId": "b47080e8-1204-49ea-82a2-d2a8533230a6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,\n",
              "        1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0], device='cuda:0')"
            ]
          },
          "execution_count": 129,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pred = pred.max(1)[1]\n",
        "pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vKnFN4YEVlxh",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vKnFN4YEVlxh",
        "outputId": "cba51e04-ba66-4284-d23a-e4fbea1106e9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,\n",
              "        1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0], device='cuda:0')"
            ]
          },
          "execution_count": 130,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZEr03kvhW_7i",
      "metadata": {
        "id": "ZEr03kvhW_7i"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5HXt3yt8Xe5U",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5HXt3yt8Xe5U",
        "outputId": "45e0e8b2-9ddf-4afb-d27f-24418ca396a3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,\n",
              "        1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0])"
            ]
          },
          "execution_count": 132,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_true = y_test.to('cpu')\n",
        "y_pred = pred.to('cpu')\n",
        "y_true"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UB7AZi5LavYR",
      "metadata": {
        "id": "UB7AZi5LavYR"
      },
      "outputs": [],
      "source": [
        "y_true = y_true.numpy()\n",
        "y_pred = y_pred.numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "U28utq--YsFO",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U28utq--YsFO",
        "outputId": "1d7ca13e-7616-4d0f-dabf-0d5bcf82acf7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[21,  1],\n",
              "       [ 0, 18]])"
            ]
          },
          "execution_count": 134,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cf_matrix = confusion_matrix(y_true, y_pred)\n",
        "cf_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cwTFJfstbz1V",
      "metadata": {
        "id": "cwTFJfstbz1V"
      },
      "outputs": [],
      "source": [
        "classes = ['Not Counterfiet','Counterfiet']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "M0UU1S4faUQR",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "M0UU1S4faUQR",
        "outputId": "6a962f44-c8fa-4c8a-fff2-e43661f39ddd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<Axes: >"
            ]
          },
          "execution_count": 136,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 864x504 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "df_cm = pd.DataFrame(cf_matrix / np.sum(cf_matrix, axis=1)[:, None], index = [i for i in classes],\n",
        "                     columns = [i for i in classes])\n",
        "plt.figure(figsize = (12,7))\n",
        "sn.heatmap(df_cm, annot=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "iGHMY6gEb5aC",
      "metadata": {
        "id": "iGHMY6gEb5aC"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
